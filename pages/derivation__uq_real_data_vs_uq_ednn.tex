\pagebreak

\section*{Herleitung der Unsicherheitsäquivalenz zwischen Evidential Neural Networks und GUM}
\label{sec:enn_gum_derivation}

Die Messunsicherheit gemäß GUM (Guide to the Expression of Uncertainty in Measurement) wird im Kern als Varianz des Messwerts verstanden: $u(x)^2 = \mathrm{Var}(x) = \mathbb{E}[(x - \mu)^2]$. Messunsicherheiten werden meist durch Konfidenzintervalle angegeben: $x \pm k \cdot u(x)$, wobei $k$ das Vertrauensniveau bestimmt.

\textbf{Ausgabe eines Evidential Neural Network.} Ein \gls{Evidenzbasierte neuronale Netze} liefert keine einzelne Punktvorhersage, sondern eine Wahrscheinlichkeitsverteilung, oft eine Normal-Inverse-Gamma-Verteilung (NIG). Es schätzt Parameter $\mu$ (Mittelwert), $\nu$ (Evidenz über die Präzision) sowie $\alpha, \beta$ (Parameter der Inverse-Gamma-Verteilung für $\sigma^2$). Die bedingte Verteilung über $y$ lautet: $p(y \mid \mu, \alpha, \beta, \nu) = \int \mathcal{N}(y \mid \mu, \sigma^2/\nu) \cdot \mathrm{InvGamma}(\sigma^2 \mid \alpha, \beta)\, d\sigma^2$. Diese Mischverteilung führt zu einer Student-$t$-Verteilung für $y$.

\textbf{Varianz eines Evidential Neural Network.} Die Vorhersagevarianz (Predictive Variance) ergibt sich zu $\mathrm{Var}(y) = \mathbb{E}[\sigma^2] + \frac{\beta}{(\alpha - 1)\, \nu}$ mit $\mathbb{E}[\sigma^2] = \frac{\beta}{\alpha - 1}$. Die Vorhersageunsicherheit lässt sich schreiben als $u(y)^2 = \underbrace{\mathbb{E}[\sigma^2]}_{\text{aleatorisch}} + \underbrace{\frac{\mathbb{E}[\sigma^2]}{\nu}}_{\text{epistemisch}}$.

\textbf{Äquivalenz zu GUM.} Dies entspricht exakt der GUM-Definition, die die Gesamtmessunsicherheit als Summe verschiedener Unsicherheitsquellen beschreibt. Somit kann die Varianz eines \gls{Evidenzbasierte neuronale Netze} direkt als Messunsicherheit gemäß GUM interpretiert werden. Das zugehörige Konfidenzintervall lautet: $\bigl[y_\text{pred} - k \cdot u(y), \; y_\text{pred} + k \cdot u(y)\bigr]$.

\textbf{Zusammenfassung.} Die in Evidential Neural Networks modellierte Predictive Variance entspricht der Messunsicherheit nach GUM, da sie sowohl aleatorische Streuung als auch epistemische Unsicherheit integriert. Damit lassen sich Unsicherheiten eines \gls{Evidenzbasierte neuronale Netze} direkt in die standardisierte Unsicherheitsangabe eines Messprozesses überführen: $u(y)^2 = \frac{\beta}{\alpha - 1} + \frac{\beta}{(\alpha - 1)\, \nu}$.
