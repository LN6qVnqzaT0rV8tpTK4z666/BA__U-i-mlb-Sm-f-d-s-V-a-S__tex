% !TeX root = ../main.tex

\chapter{Durchführung}\label{chapter:durchfuehrung}



\begin{otherlanguage}{american}
  
\end{otherlanguage}



\begin{otherlanguage}{ngerman}

In der Durchführung soll \textit{A6 Implementierung und Validierung der entwickelten Unsicherheitsmetriken
und Modellierungstechniken in einer realistischen Anwendung, wie z. B. der
Unsicherheitsabschätzung in kritischen Regionen von Simulationsdaten} beantwortet werden. \parencite{amini2020deep} liefert einen bekannten validierten Ansatz aus öffentlicher Forschung. Hergert 2025 liefert einen bekannten validierten Ansatz von Unternehmensseite. 

\section*{Bekannte Ansätze}

Zur Validierung der eingesetzten Methodik für \gls{Evidenzbasierte neuronale Netze} wird der von \parencite{amini2020deep} veröffentlichte Benchmark reproduziert. Hierfür wird die im Anhang dargestellte Aufwandsschätzung berücksichtigt, die auf die im Unternehmen verfügbare Hardware abgestimmt ist, (~\ref{sec:aufwand-edl}).

\paragraph{Vergleich Amini \gls{Evidenzbasierte neuronale Netze} Regression} \parencite{amini2020deep} Sektion 4 Experiment liefert Ergebnisse für \gls{Evidenzbasierte neuronale Netze}.  
\begin{noindentquote}
\scriptsize
$
\{ \text{RMSE}, \text{NLL} \} 
\times 
\{ \text{dropout}, \text{ensembles}, \text{evidential} \} 
\times 
\{ \text{boston}, \text{concrete}, \text{energy}, \text{kin8nm}, \text{naval}, \text{power}, \text{protein}, \text{wine}, \text{yacht} \}
$  
\end{noindentquote}
Der Benchmark konnte in erster Konfiguration annähernd reproduziert werden. 
\begin{noindentquote}
  \scriptsize
  \texttt{python3 neurips2020/run\_uci\_dataset\_tests.py --datasets boston concrete energy-efficiency kin8nm naval power-plant protein wine yacht --num-trials 1 --num-epochs 5}
\end{noindentquote}
 mit Laufzeit von 9 Minuten 10.2 Sekunden $\leq$ 10 Minuten als \glqq Proof of concept\grqq. Die Konvergenz zu Aminis Werten ($\Delta (\text{RMSE}) = \left[ -0.5;\ -0.6 \right]$; $\Delta \text{NLL} = -0.08$) wird deutlich für die Konfiguration hinreichend umfangreicher Datensätze $\{\text{kin8nm}, \text{naval}, \text{protein} \}$. ~\ref{tab:comparison_ours_vs_amini_tabularx_v2} unter der Hardware-Limitierung aus Tabelle 1, Gerät 1, Hardware-Spezifikation, Anhang. % Für den visuell Abgleich siehe Abschnitt~\ref{sec:visualisierungen_edl_amini2020_regression}

\begin{table*}[!htbp]
\centering
\scriptsize
\begin{tabularx}{\textwidth}{|l|X|X|X|X|X|X|}
\hline
\multirow{}{}{\textbf{Datensatz}} 
& \multicolumn{3}{c|}{\textbf{RMSE}} 
& \multicolumn{3}{c|}{\textbf{NLL}} \\ \cline{2-7}
& \textbf{Dropout} & \textbf{Ensemble} & \textbf{Evidential} 
& \textbf{Dropout} & \textbf{Ensemble} & \textbf{Evidential} \\ \hline
boston 
& 3.678 \newline \textit{2.896} \newline \mbox{$\Delta=+0.782$} 
& 4.956 \newline \textit{2.998} \newline \mbox{$\Delta=+1.958$} 
& 3.672 \newline \textit{2.886} \newline \mbox{$\Delta=+0.786$} 
& 4.636 \newline \textit{2.755} \newline \mbox{$\Delta=+1.881$} 
& 2.981 \newline \textit{2.838} \newline \mbox{$\Delta=+0.143$} 
& 2.621 \newline \textit{2.701} \newline \mbox{$\Delta=-0.080$} \\ \hline
concrete 
& 6.905 \newline \textit{5.367} \newline \mbox{$\Delta=+1.538$} 
& 6.705 \newline \textit{5.263} \newline \mbox{$\Delta=+1.442$} 
& 6.496 \newline \textit{5.163} \newline \mbox{$\Delta=+1.333$} 
& 5.254 \newline \textit{3.213} \newline \mbox{$\Delta=+2.041$} 
& 3.271 \newline \textit{3.193} \newline \mbox{$\Delta=+0.078$} 
& 3.277 \newline \textit{3.138} \newline \mbox{$\Delta=+0.139$} \\ \hline
energy-efficiency 
& 2.721 \newline \textit{1.200} \newline \mbox{$\Delta=+1.521$} 
& 2.516 \newline \textit{0.987} \newline \mbox{$\Delta=+1.529$} 
& 2.487 \newline \textit{0.984} \newline \mbox{$\Delta=+1.503$} 
& 4.699 \newline \textit{-0.964} \newline \mbox{$\Delta=+5.663$} 
& 1.920 \newline \textit{-1.315} \newline \mbox{$\Delta=+3.235$} 
& 1.879 \newline \textit{-1.376} \newline \mbox{$\Delta=+3.255$} \\ \hline
kin8nm 
& 0.095 \newline \textit{0.096} \newline \mbox{$\Delta=-0.001$} 
& 0.084 \newline \textit{0.080} \newline \mbox{$\Delta=+0.004$} 
& 0.088 \newline \textit{0.077} \newline \mbox{$\Delta=+0.011$} 
& 1.114 \newline \textit{-1.213} \newline \mbox{$\Delta=+2.327$} 
& -1.113 \newline \textit{-1.496} \newline \mbox{$\Delta=+0.383$} 
& -0.992 \newline \textit{-1.537} \newline \mbox{$\Delta=+0.545$} \\ \hline
naval 
& 0.005 \newline \textit{0.003} \newline \mbox{$\Delta=+0.002$} 
& 0.005 \newline \textit{0.002} \newline \mbox{$\Delta=+0.003$} 
& 0.005 \newline \textit{0.002} \newline \mbox{$\Delta=+0.003$} 
& -2.438 \newline \textit{-6.233} \newline \mbox{$\Delta=+3.795$} 
& -3.931 \newline \textit{-6.967} \newline \mbox{$\Delta=+3.036$} 
& -3.747 \newline \textit{-6.987} \newline \mbox{$\Delta=+3.240$} \\ \hline
power-plant 
& 3.349 \newline \textit{4.049} \newline \mbox{$\Delta=-0.700$} 
& 3.467 \newline \textit{4.021} \newline \mbox{$\Delta=-0.554$} 
& 3.377 \newline \textit{3.987} \newline \mbox{$\Delta=-0.610$} 
& 5.281 \newline \textit{2.795} \newline \mbox{$\Delta=+2.486$} 
& 2.678 \newline \textit{2.765} \newline \mbox{$\Delta=-0.087$} 
& 2.671 \newline \textit{2.755} \newline \mbox{$\Delta=-0.084$} \\ \hline
protein 
& 4.355 \newline \textit{4.512} \newline \mbox{$\Delta=-0.157$} 
& 3.939 \newline \textit{4.480} \newline \mbox{$\Delta=-0.541$} 
& 4.169 \newline \textit{4.430} \newline \mbox{$\Delta=-0.261$} 
& 4.266 \newline \textit{2.871} \newline \mbox{$\Delta=+1.395$} 
& 2.742 \newline \textit{2.837} \newline \mbox{$\Delta=-0.095$} 
& 2.706 \newline \textit{2.779} \newline \mbox{$\Delta=-0.073$} \\ \hline
wine 
& 0.826 \newline \textit{0.648} \newline \mbox{$\Delta=+0.178$} 
& 0.829 \newline \textit{0.631} \newline \mbox{$\Delta=+0.198$} 
& 0.722 \newline \textit{0.609} \newline \mbox{$\Delta=+0.113$} 
& 2.255 \newline \textit{-0.574} \newline \mbox{$\Delta=+2.829$} 
& 1.390 \newline \textit{-0.618} \newline \mbox{$\Delta=+2.008$} 
& 1.187 \newline \textit{-0.679} \newline \mbox{$\Delta=+1.866$} \\ \hline
yacht 
& 7.046 \newline \textit{1.711} \newline \mbox{$\Delta=+5.335$} 
& 9.197 \newline \textit{1.337} \newline \mbox{$\Delta=+7.860$} 
& 5.557 \newline \textit{1.252} \newline \mbox{$\Delta=+4.305$} 
& 5.170 \newline \textit{-1.637} \newline \mbox{$\Delta=+6.807$} 
& 3.041 \newline \textit{-2.257} \newline \mbox{$\Delta=+5.298$} 
& 2.401 \newline \textit{-2.341} \newline \mbox{$\Delta=+4.742$} \\ \hline
\end{tabularx}
\caption{Vergleich reproduzierte Ergebnisse mit Benchmark \textit{Amini et al. (2020)}}
\label{tab:comparison_ours_vs_amini_tabularx_v3}
\end{table*}


\paragraph{Vergleich Amini \gls{Evidenzbasierte neuronale Netze} Klassifikation} \parencite{amini2020deep} \texttt{gen\_depth\_results.py} liefert Ergebnisse für \gls{Evidenzbasierte neuronale Netze} für die Aufgabe \glqq Monocular depth estimation\grqq. Der Benchmark konnte reproduziert werden mit einer Konfiguration $N_\text{batch}=25$, $N_\text{adv}=3$ unter der Hardware-Limitierung aus Tabelle 1, Gerät 1, Hardware-Spezifikation, Anhang. % Für den visuell Abgleich siehe Abschnitt~\ref{sec:visualisierungen_edl_amini2020_classification}



\section*{Eigenentwicklung}

Benötigt wurde eine Datenpipeline. In Entwicklung mit Herr M.Sc. N. Mavani sollte für den Vergleich von \gls{Bayesianische neuronale Netze}, \gls{Evidenzbasierte neuronale Netze} als Baseline von Mavanis Seite auf \parencite{Depeweg2019}, von meiner Seite auf \parencite{Ulmer2023} aufgesetzt werden.\newline

Dazu wurden von \parencite{Depeweg2019} verwendete Datensätze verwendet und ergänzt um in \hyperref[sec:datensaetze]{Datensätze} gelistete. Aufgesetzte Stufen der Datenpipeline sind eine Vorverarbeitung zum Umwandeln von Formaten, Einlesen in Torch Dataset Klassen, Torch Trainingsklassen, Visualisierungsklassen.\newline

Aus den Datensätzen wurden Trainings und Visualisierungen abgeleitet und generifiziert. Ebenso wurden recherchierte Loss-Funktionen generifiziert. Trainings wurden auf einen modernen Standard ergänzt wie Checkpoints, Patience-Epochen, vergleichbar und generifiziert. Nach Generifizierung bleiben einer Klasse überwiegend nur noch konfigurierbare Anteile vorbehalten. Eine Konfiguration wurde als Refactor geplant und soll von dort aus zu einer zentralen Konfiguration hochgereicht werden.\newline

Im Projekt behandelte Metriken wurden aufgefasst mit einer Klasse Metrik, Metrik-Thresholds, Metrik-Registry, Metrik-Registry-Defintions deren Name die Funktion hinreichend vorgibt.\newline

Integriert wurden Lösungen von Herr M.Sc. Mavani der sich zur \gls{Bayesianische neuronale Netze} Auswertung für ein Python Pyro Jax Backend entschieden und damit Lösungen nach den Pyroworkflows für Hamilton Monte Carlo (\gls{hmc}), Stochastic Variational Inference (\gls{svi}) geliefert hat.


% \subsection*{Experiment}

\begin{table}[htbp]
\centering
\begin{tabularx}{\textwidth}{|l|l|l|l|l|l|l|X|}
\hline
 & \texttt{nll} & \texttt{abs} & \texttt{mse} & \texttt{kl} & \texttt{scaled} & \texttt{variational} & \texttt{full} \\
\hline
EDNN Accuracy & & & & & & & \\
\hline
BNN Accuracy & & & & & & & \\
\hline
\end{tabularx}
\caption{Zusammenfassung der Evidential Loss Varianten}
\end{table}


\section*{Visueller Abgleich \gls{Bayesianische neuronale Netze}-\gls{Evidenzbasierte neuronale Netze}}

\begin{figure}[!h]
  \centering
  \begin{minipage}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../figures/bnn_posterior_predictive.png}
      \caption{Ninand Mavani Visual Benchmark \gls{Bayesianische neuronale Netze} \gls{hmc}}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../figures/bnn_svi_predictive.png}
      \caption{Ninand Mavani Visual Benchmark \gls{Bayesianische neuronale Netze} \gls{svi}}
  \end{minipage}
\end{figure}


% \subsection*{Schnittstelle}

% \subsection*{Benchmark EDNN}

% \subsection*{Benchmark BNN-EDNN}

% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Sigma (Standard Deviation)} & \textbf{BNN Variance} & \textbf{BNN Accuracy (\%)} & \textbf{EDNN Variance} & \textbf{EDNN Accuracy (\%)} \\
% \hline
% 0 & 0.0 & 0 & 0.0 & 0 \\
% \hline
% \end{tabular}
% }
% \caption{Vergleich von BNN und EDNN mit Meta-Metrik im 2-Sigma-Bereich}
% \label{tab:BNN_EDNN_Comparison}
% \end{table}


\subsection*{VaMai - Kritikalitätsraum - Validierung - Annahmen}

Vamai verfolgt mit dem Kritikalitätsraum die Idee einer Kritikalitätsabdeckung. Umgangssprachlich möchte man den Raum so gut abdecken, dass mit einer gewissen Wahrscheinlichkeit kein Kritikalitätsbereich mehr in diesem Raum vorhanden ist. Als Beweisidee äußert sich das wie folgt: Zeige, dass durch die gewählte Methode (z.\,B. Evidential Deep Learning und Unsicherheitsaggregation) die Wahrscheinlichkeit dafür, dass ein unentdeckter Kritikalitätsraum existiert, unter einem Schwellenwert liegt.

\[ 
\mathbb{P}\bigl(\exists\, x \in \Omega \setminus \widehat{\Omega}_k : \text{Kritikalität}(x) > \tau \bigr) < \varepsilon. 
\]

Das heißt, die Wahrscheinlichkeit, dass irgendwo im Raum $\Omega$ noch ein Punkt existiert, dessen Kritikalität den Schwellenwert überschreitet, aber nicht durch diese Methode erkannt wurde, ist hinreichend klein (z.\,B. unter 5\,\% mit $\mu \pm 2\sigma$). $\Omega \subset \mathbb{R}^{n}$ ist der gesamte Zustands- oder Szenarienraum. $\Omega_k := \left\{ x \in \Omega \;\big|\; k(x) \ge \tau \right\}$ ist der echte Kritikalitätsraum. $\widehat{\Omega}_k := \left\{ x \in \Omega \;\big|\; k(x) \ge \tau \right\}$ ist der durch die Methode erkannte Kritikalitätsraum. $\varepsilon \in (0,1)$ ist das akzeptierte Fehlerniveau.

Zu beweisen ist: $\mathbb{P}\bigl( \Omega_k \setminus \widehat{\Omega}_k \neq \emptyset \bigr) \le \varepsilon.$ Dazu wird folgende Vorgehensweise in Betracht gezogen: 

% enger formalisierbar, siehe Notizen

\begin{enumerate}
  \item Unsicherheitsmodellierung mittels Evidential Deep Learning
  \begin{itemize}
    \item Liefert für jeden Punkt $x \in \Omega$ eine Wahrscheinlichkeitsverteilung mit Varianz $\mathrm{Var}(x) = \sigma^{2}_{\text{epistemisch}} + \sigma^{2}_{\text{aleatorisch}}$.
    \item Kritikalität ist Funktion der Größe $k(x)$.
  \end{itemize}
  \item Sampling- / Klassifikationsabdeckung
  \begin{itemize}
    \item Raum $\Omega$ wurde mit Dichte $\delta$ gesampled (zum Beispiel mittels Gitter, Trajektorien, Pfaden) von VaMai Projektteilnehmer B.Sc. Pascal Imholze mit dem Ziel einer möglichst hohen Abdeckung.
    \item Wenn $\forall x \in \Omgea$ ein Sample $x' \in \widehat{\Omega}_k$ mit $\lVert x - x' \rVert \le \delta'$ existiert, gilt \glqq dichte Abdeckung \grqq.
    \item Generell ist in Hinblick auf in VaMai abgezielte Methoden wie \glq Few Shot Learning \grqq von Herr Mavani mit Sampleumfängen $n \in [20, 30], n \in \mathbb{N}$ zu beachten, dass der zentrale Grenzwertsatz der auch die \gls{EvidentialDeepLearning} später erläuterte Student-t-Verteilung betrifft, nach Faustformel ab einem Stichprobenumfang $n \geq 30$ gültig ist. % Quelle GUM
  \end{itemize}
  \item Uniforme Konvergenz / PAC-ähnliche Argumentation
  \begin{itemize}
    \item Zeige, dass mit Vorhersagewahrscheinlichkeit, Unsicherheitsabschätzung die Methodik mit zunehmender Samplezahl $N$ gegen den wahren Kritikalitätsraum konvergiert. 
    \[
    \mathbb{P}\bigl( \sup_{x \in \Omega} \bigl\lvert f(x) - \widehat{k}(x) \bigr\rvert \ge \eta \bigr) \le \varepsilon.
    \]
    \item Wenn $\widehat{k(x)} \approx k(x)$ auf ganz $\Omega$, gilt auch $\widehat{\Omega}_{k} \approx \Omega_{k}$. Es folgt, dass die Wahrscheinlichkeit einen echten Kritikalitätsraum \textit{nicht} zu erkennen kleiner als $\epsilon$ ist.
  \end{itemize}

\end{enumerate}

\subsection*{VaMai - Kritikalitätsraum - Validierung - Experiment}

Herr Autenrieths Kritikalitätsraum-Software wurde eingebunden in die bisherige Eigenentwicklung. Die Umgebung liefert einen Raum in dem beispielhafte Signalumgebungen 
% evtl. formalisieren.
zusammengesetzt werden können. Er ist bereits grundlegend unsicherheitsbehaftbar. Zuerst wurde eine Visualisierung nach Amini Deep Evidential Regression 2020 ergänzt. Als nächstes wurde ein \gls{EvidentialDeepLearning}-Training unter Aminis vorhandenen bereits optimierten Modellen (siehe Rekonstruktion Aminis Ergebnisse) in den Hauptablauf integriert. 

Die synthetischen Daten ersetzen in diesem Fall reale Messungen, Simulationsergebnisse $\{ x_{i}, y_{i} \}$. $x_{i}$ ist ein Eingabepunkt der als kleinste Einheit bereits als Szenario im Parameterraum zählen kann. $y_{i}$ liefert die Kritikalität in diesem Punkt. Das \gls{EvidentialDeepLearning}-Modell beantwortet die Frage, mit welcher Unsicherheit es glaube, dass $y$ an einem neuen Punkt läge. 

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{../figures/x4.png}
  \caption{R4.4 Angen. Grund-Verteilung auf unbek. Daten für \gls{EvidentialDeepLearning}-Modell n. Amini 2020}
\end{figure}

Das evidentielle Modell nach Amini ordnet das als $y \sim \mathcal{N}(\mu, \sigma^2)$ zu. $(\mu$, $\sigma^2)$ kennt das Modell nicht aus Vorwissen. Es nimmt stattdessen an, erstens $\mu$ ist unsicherheitsbehaftet, damit folgt $\mu \sim \mathcal{N}(\gamma, \sigma^{2} / \lambda)$. Zweitens $\sigma^{2}$ ist unsicherheitsbehaftet, damit folgt $\sigma^{2} \sim \mathcal{NIG}(\alpha, \beta)$. So sucht das Modell $p(y|\gamma,\lambda,\alpha,\beta)$. Die Daten liefern die Evidenz als Ergebnis. Das \gls{EvidentialDeepLearning}-Modell schätzt direkt die Hyperparameter ausgehend von Eingaben $x$, $f(x) \rightarrow \{ \gamma(x), \lambda(x), \alpha(x), \beta(x) \}$. Es ergibt sich für jeden Punkt $x$ eine bedingte Unsicherheitsverteilung für $y$. Amini verdeutlicht dies kompakt in Gleichungen (3), (4), (5), Abbildung R.4.4 (vgl. Amini 2020 \parencite{amini2020}). 

Ergebende Unsicherheitsmasse ist Student-t-verteilt (vgl. Gleichungen (6), (7)), damit ist Mittelwert $\mu$ als \glqq{}Position\grqq{}, Skalierung $\sigma^{2}$ als \glqq{}Streuung\grqq{}, Freiheitsgrad $\nu$ als \glqq{}Ordnung\grqq{} bekannt. 

\[
f : \mathbb{R}^d \to \mathbb{R}^4, \quad x \mapsto (\gamma, \lambda, \alpha, \beta) \quad \Rightarrow \quad p(y \mid x) \sim \text{Student-}t\left(\mu = \gamma,\, \sigma^2 = \frac{\alpha \lambda}{\beta (1 + \lambda)},\, \nu = 2\alpha\right)
\]

R4.4 visualisiert die Zusammensetzung der Student-\emph{t}-Verteilung in Abhängigkeit von der Evidenz im Ausdruck der Ordnung: Bei niedriger Ordnung sowie Evidenz (\(\nearrow \alpha \Rightarrow \nearrow \nu\)) ergibt sich eine breite, unsichere Verteilung. Während höhere Ordnung sowie Evidenz (\(\searrow \alpha \Rightarrow \searrow \nu\)) zu einer schmaleren, sichereren Verteilung führt, welche gegen die Normalverteilung konvergiert. Alle weiteren Proportionalitäten für die evidentiellen Parameter sind in (5) einsehbar (vgl. Amini 2020 \parencite{amini2020}). Erste vom Modell geschätze Wahrscheinlichkeitsmasse um einen Schätzwert ergibt sich aus Gleichung (4) ohne Regularisierung in R4.4. platikurtisch hellblau. Regularisierungsbedingungen formen die Verteilung unter Evidenz-Kriterien leptokurtisch dunkelblau. Machine-Learning-basiert regularisieren Loss-Funktion nach dem Min-Max-Prinzip angenommene Fehler- gegen Zielgrößen, hier durch Amini beschränkt Varianz gegenüber Evidenz (vgl. Gleichung (8), (9), (10) unter (5)). Amini wählt die Kullback-Leibler-Divergenz als Maß für die Ausgangs- zur Optimalverteilung. Überlässt dabei jedoch ein Detail dem geneigten Leser, dessen Behandlung im Stand der Technik mittels Josangs \gls{subjectivelogic} aufgezeigt wurde.

\begin{quote}
  \glqq{}[..] the KL between any NIG and the zero evidence NIG prior is undefined.\grqq{} \newline ~\parencite[{S. 5, \glqq Minimizing evidence on errors \grqq, Z. 6 ff.}]{amini2020deep}
\end{quote}

Herr Autenrieths Kritikalitätsräume erlauben folgende Parametrisierung: 

\begin{itemize}
  \item \textbf{Gaussverteilung:}
    \[
    f(\mathbf{x}) = A \cdot \exp\left(-\frac{1}{2} (\mathbf{x} - \boldsymbol{\mu})^\top \mathbf{\Sigma}^{-1} (\mathbf{x} - \boldsymbol{\mu}) \right)
    \]
    \text{mit}:
    \begin{itemize}
      \item $\boldsymbol{\mu} \in \mathbb{R}^d$: Mittelwert
      \item $\mathbf{\Sigma} = \mathrm{diag}(\boldsymbol{\sigma}^2)$ mit $\boldsymbol{\sigma} \in \mathbb{R}_{>0}^d$: Standardabweichungen
      \item $A > 0$: maximale Amplitude (optional)
      \item \text{range\_factor} $\in \mathbb{R}_{>0}$: Ausdehnung (optional)
      \item \text{active\_dims}: aktive Eingabedimensionen $\subseteq \{1, \dots, D\}$
    \end{itemize}

  \item \textbf{Statischer Hyperwürfel:}
    \[
    f(\mathbf{x}) =
    \begin{cases}
      c, & \text{wenn } \mathbf{x} \in \prod_{i=1}^d [a_i, b_i] \\
      0, & \text{sonst}
    \end{cases}
    \]
    \text{mit:}
    \begin{itemize}
      \item $[a_i, b_i]$: Begrenzung pro Dimension
      \item $c \in \mathbb{R}$: Konstanter Wert
    \end{itemize}

  \item \textbf{Rampe:}
    \[
    f(\mathbf{x}) = \mathrm{clip}\left( \sum_{i=1}^d \alpha_i x_i, \text{min}, \text{max} \right)
    \]
    \text{mit:}
    \begin{itemize}
      \item $\boldsymbol{\alpha} \in \mathbb{R}^d$: Koeffizienten
      \item \text{bounds}: $[a_i, b_i]$ für jede aktive Dimension
      \item \text{min\_value}, \text{max\_value}: Begrenzung des Funktionswerts
    \end{itemize}

  \item \textbf{Rauschen:}
    \[
    f(\mathbf{x}) = A + \mathcal{N}(0, \sigma^2)
    \]
    \text{mit:}
    \begin{itemize}
      \item $A$: Basiswert der Amplitude
      \item $\sigma$: Standardabweichung des Rauschens (optional)
      \item \text{seed}: zufallsbasierte Reproduzierbarkeit (optional)
    \end{itemize}

  \item \textbf{Sinus:}
    \[
    f(\mathbf{x}) = \sum_{i=1}^{n} A_i \cdot \sin(2\pi f_i \cdot \mathbf{w}_i^\top \mathbf{x} + \phi_i) + \text{offset}
    \]
    \text{mit:}
    \begin{itemize}
      \item $A_i$: Amplitude
      \item $f_i$: Frequenz
      \item $\mathbf{w}_i \in \mathbb{R}^d$: Gewichtungsvektoren
      \item $\phi_i$: Phase (optional)
      \item \text{offset}: Verschiebung nach oben/unten (optional)
      \item \text{minimum\_value}: untere Schranke (optional)
      \item \text{bounds}: Definitionsbereich (optional)
      \item \text{active\_dims}: aktive Eingabedimensionen $\subseteq \{1, \dots, D\}$
    \end{itemize}
\end{itemize}



Herr Autenrieths Kritikalitätsraums Unsicherheitsbehaftung wird zunächst äquivalent zu Aminis Modell R.4.4 refaktorisiert indem Aminis Evidential Deep Learning Repository wegen fehlender Projektstruktur direkt integriert wird. Visualisiert wird zunächst die dem Modell unbekannte ursprüngliche Unsicherheit mit der das Signal belegt ist, die aleatorische, epistemische Varianz, sowie eine Gesamtansicht mit dem ursprünglich gesampelten Kritikalitätsraum. Das VaMai Standardbeispiel liefert dazu ein Rampen-, Gauss- und multinomielles Gausssignal in einer Domäne 10 LE x 10 LE x 2 LE. Die Ausgabe des unoptimierten Modells trennt zwar \gls{Epistemische Unsicherheit} und \gls{Aleatorische Unsicherheit}. Doch die Gesamtunsicherheitsmasse des Modells kommt der wahren unterliegenden Unsicherheit ungenügend nach. Es soll als visueller Ausgangspunkt weiterverwendet werden. Die Parametrisierung wurde zwecks UI-Optimierung für kurze Ladezeit verwendet.



\paragraph{Konfiguration \glqq Prototyping \grqq} Feedforward-Netzwerk mit einer versteckten Schicht (\texttt{Dense(64, relu)}), Ausgangslayer \texttt{DenseNormalGamma}. Optimierer: Adam, Lernrate: $1\cdot10^{-3}$. Verlustfunktion: \texttt{EvidentialRegression} mit Regularisierung \texttt{coeff=1e-2}. Trainingsdauer: 100 Epochen. Trainingsdaten: Latin Hypercube Sampling (101 Samples), Rauschen: Gauß-verteiltes Rauschen mit $\sigma = 0{,}01$. Evaluation mittels RMSE auf Testdaten (80/20 Split), 5 Wiederholungen. Ausgaben: $\gamma$, $\nu$, $\alpha$, $\beta$ zur Berechnung von aleatorischer und epistemischer Varianz.

\begin{figure}[!ht]
  \centering
  \includegraphics[width=0.8\textwidth]{../figures/y1.png}
  \caption{R Grundl. Ausgangs-UI Kritikalität, Varianz total, aleat., epist.}
\end{figure}

Vereinfacht man die Signal-Parametrisierung zunächst stark zu einer Gauss-Funktion in einer Domäne 10 LE x 10 LE x 1 LE

\[
f(x, y) = A \cdot \exp\left( -\left( \frac{(x - \mu_x)^2}{2\sigma_x^2} + \frac{(y - \mu_y)^2}{2\sigma_y^2} \right) \right)
\]

\[
\text{mit:} \quad \mu_x = 5,\quad \mu_y = 5,\quad \sigma_x = 1,\quad \sigma_y = 1,\quad A = 1
\]

\[
\Rightarrow \quad f(x, y) = \exp\left( -\frac{(x - 5)^2 + (y - 5)^2}{2} \right)
\]

, wählt die unter Amini vorgeschlagene Trainings-Parametrisierung für komplexere Fälle mit zwei vollverbundenen Schichten, 128 Neuronen, 2000 Epochen, jeweils als Trainingskonfiguration 2 statt einer vollverbundenen Schicht, 64 Neuronen, 500 Epochen als Trainingskonfiguration 1, erhält man folgende zielführende Dynamik bereits ohne größere Störfunktion. 



\paragraph{Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Underfit zu annäherndem Fit} Zu sehen ist, dass bei gleicher unterlegter Varianzverteilung als Hyperwürfel sich ein Underfit für ein einfaches Signal durch die Anzahl gewählter Epochen, Layer, Anzahl Neuronen äußert. Visuell ist es insbesondere durch ein Austreten aus der ursprünglichen Signalform erkennbar. Das Ergebnis der Trainingskonfiguration 2 ist eine minimale wellenförmige Gesamtunsicherheitsmasse diagonal unter der Gesamtzahl der Samples, die durch den ganzen Raum von den Minima über das Maxima des ursprünglichen Signals gesetzt wird. Es wird deutlich, dass Aminis Vorschlag der Trainingskonfiguration 2 für zweidimensionale Fälle im dreidimensionalen Fall mindestens durch Trainingskonfiguration 1 ersetzt werden sollte, um Ergebnisse zu erhalten, welche dem Original-Signal als Ground Truth in der Verteilung nahe kommen. Unter der Trainingskonfiguration 1 lässt sich behaupten, dass vorliegendes einfaches Gauss-Signal durch die erkannten Unsicherheiten in seiner Originalform annähernd bestimmt werden kann. Zwei Schichten, 128 Neuronen werden in recherchierter Literatur für komplexe Fälle erwähnt. % Quelle

% \begin{figure}[!ht]
%   \centering
%   \begin{subfigure}[t]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{../figures/y21.png}
%     \caption{Konfiguration 1, Original-Signal als Wert der Kritikalitätsfunktion k(..) als \gls{GroundTruth}}
%     \label{fig:bild21}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[t]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{../figures/y22.png}
%     \caption{Konfiguration 1, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
%     \label{fig:bild22}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[t]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{../figures/y23.png}
%     \caption{Konfiguration 1, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
%     \label{fig:bild23}
%   \end{subfigure}
%   \label{fig:three_subfigures1}
% \end{figure}

\begin{figure}[!ht]
  \centering

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y21.png}
    \caption{Konfiguration 1, Original-Signal als Wert der Kritikalitätsfunktion \(k(\cdot)\) als \gls{GroundTruth}}
    \label{fig:bild21}
  \end{subfigure}

  \vspace{0.5em}  % Kleiner Abstand zwischen den Bildern

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y22.png}
    \caption{Konfiguration 1, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
    \label{fig:bild22}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y23.png}
    \caption{Konfiguration 1, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
    \label{fig:bild23}
  \end{subfigure}

  \caption{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 1}
  \label{fig:three_subfigures1}
\end{figure}



% \begin{figure}[!ht]
%   \centering
%   \begin{subfigure}[t]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{../figures/y31.png}
%     \caption{Konfiguration 2, Original-Signal als Wert der Kritikalitätsfunktion k(..) als \gls{GroundTruth}}
%     \label{fig:bild31}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[t]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{../figures/y32.png}
%     \caption{Konfiguration 2, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
%     \label{fig:bild32}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[t]{0.31\textwidth}
%     \centering
%     \includegraphics[width=\textwidth]{../figures/y33.png}
%     \caption{Konfiguration 2, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
%     \label{fig:bild33}
%   \end{subfigure}
%   \label{fig:three_subfigures2}
% \end{figure}

\begin{figure}[!ht]
  \centering

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y31.png}
    \caption{Konfiguration 2, Original-Signal als Wert der Kritikalitätsfunktion \(k(\cdot)\) als \gls{GroundTruth}}
    \label{fig:bild31}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y32.png}
    \caption{Konfiguration 2, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
    \label{fig:bild32}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y33.png}
    \caption{Konfiguration 2, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
    \label{fig:bild33}
  \end{subfigure}

  \caption{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 2}
  \label{fig:three_subfigures2}
\end{figure}

\paragraph{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 2} 



\paragraph{Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Overfit} Zu sehen ist, dass bei gleicher unterlegter Varianzverteilung als Hyperwürfel sich ein Overfit für ein einfaches Signal durch die doppelte Anzahl gewählter Epochen bei gleicher Anzahl Layer, Anzahl Neuronen wie im Fit-Szenario äußert. Visuell ist es insbesondere durch ein Austreten der epistemischen Varianz aus der ursprünglichen Signalform erkennbar. Das Ergebnis beruht auf der Trainingskonfiguration 1 mit doppelter Epochenzahl von viertausend. Es wird visuell deutlich, dass die Ground Truth der epistemischen Varianz im Original-Signal, welche durch die Obergrenze bei eins limitiert ist, durch einen Overfit überkonfident auf 1,4 approximiert wird. Der vorhandene Sample-Punkt auf der Spitze des Gauss-Signals als gesampletes Maximum des Original-Signals wird überschritten. Eine optimale Epochenzahl für einen Fit für das einzelne 3D-Gauss-Signal lässt sich so auf den Bereich zwischen 2000 und 4000 Epochen mindestens aus diesen Try-And-Error-Versuchen eingrenzen.



\begin{figure}[!ht]
  \centering

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y41.png}
    \caption{Konfiguration 3, Original-Signal als Wert der Kritikalitätsfunktion \(k(\cdot)\) als \gls{GroundTruth}}
    \label{fig:bild31}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y42.png}
    \caption{Konfiguration 3, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
    \label{fig:bild32}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y43.png}
    \caption{Konfiguration 3, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
    \label{fig:bild33}
  \end{subfigure}

  \caption{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 3}
  \label{fig:three_subfigures3}
\end{figure}



\paragraph{Multi 3D-Signal \gls{EvidentialDeepLearning} Underfit zu annäherndem Fit} 

Folgendes Multi-3D-Signal zweier bereits mit Unsicherheit belegter Gauss-Signale 

\[
F(x, y) = 2 \cdot \exp\left( -\frac{(x - 9)^2}{2 \cdot 1^2} - \frac{(y - 5)^2}{2 \cdot 1^2} \right) + \exp\left( -\frac{(x - 2)^2}{2 \cdot 0.5^2} - \frac{(y - 2)^2}{2 \cdot 0.5^2} \right) + \exp\left( -\frac{(x - 7)^2}{2 \cdot 1^2} - \frac{(y - 7)^2}{2 \cdot 1^2} \right)
\]

erzielte auf bisheriger Grundlage mit folgenden Parametrisierungen Underfits. 

\begin{table}[h!]
\centering
\begin{tabular}{|c|c|c|}
\hline
\textbf{Schichten} & \textbf{Neuronen pro Schicht} & \textbf{Epochen} \\
\hline
4 & 128 & 8000 \\
2 & 256 & 4000 \\
2 & 256 & 2000 \\
\hline
\end{tabular}
\caption{Vergleich von Netzwerkarchitekturen und Trainingsparametern}
\end{table}

Zu beobachten ist, dass eine Erhöhung der Schicht- und Neuronen-Anzahl im 3D-Fall dazu beiträgt das größte Teil-Signal präziser auf dem Domänenboden abzubilden bis es die Originalposition des größten Teil-Signals erreicht. Jedoch fällt das Volumen zu flach aus. Eine weitere Beobachtung ist, dass Aminis Regressionsansatz bei mehreren Signalformen in der Domäne, im Volumen die Gesamtunsicherheitsmasse der aleatorischen und epistemischen Varianz aller auftretenden Signale erfasst. Ergänzend dazu orientiert sich die Position der erfassten Gesamtunsicherheitsmasse jedoch im ersten Fall an der größten Dichte von Maximalwerten der auftretenden Signal-Formen. 

Wählt man schließlich der ursprünglichen Parametrisierung von Amini entgegenkommend

2 Schichten, 64 Neuronen, 2000 Epochen 

kommt man zum besten gemittelten Ergebnis aus der aleatorischen und epistemischen Varianz zwischen den beiden verwendeten Signalen. 

Für weitere Schritte wird das wie folgt interpretiert. Im Projekt VaMai möchte man die aleatorische und epistemische Varianz pro Signal präziser erfassen, sodass eben geschilderte Beobachtungen nicht hinreichend sind. 

Lösungsansätze könnten beispielsweise das Eingrenzen von Signal-Positionen in der Domäne durch eine beliebige Abtastung sein. Das abgetastete Teilvolumen wird dann vom Evidentiellen Netzwerk auf seine Teilunsicherheiten untersucht. Das Ergebnis setzt man additiv zusammen. 

Die Erwartungshaltung ist, dass sich so, sowohl die Position der Teil-Unsicherheitsmassen als auch die Volumen der Teil-Unsicherheitsmassen in Bezug auf die als Boden angesehene Fläche der verglichenen Dimensionen korrekt bestimmen lassen. 



\begin{figure}[!ht]
  \centering

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y51.png}
    \caption{Konfiguration 4, Original-Signal als Wert der Kritikalitätsfunktion \(k(\cdot)\) als \gls{GroundTruth}}
    \label{fig:bild31}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y52.png}
    \caption{Konfiguration 4, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
    \label{fig:bild32}
  \end{subfigure}

  \vspace{0.5em}

  \begin{subfigure}[t]{\textwidth}
    \centering
    \includegraphics[width=0.6\textwidth]{../figures/y53.png}
    \caption{Konfiguration 4, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
    \label{fig:bild33}
  \end{subfigure}

  \caption{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 4}
  \label{fig:three_subfigures3}
\end{figure}



\paragraph{Multi 3D-Signal \gls{EvidentialDeepLearning} Overfit} Es wurde im Zeitrahmen unter im letzten Paragraphen erwähnter Parametrisierung kein offensichtlicher Overfit erzielt, der alle Signal-Klassen visuell detailgetreu abbildet. Interpretiert wird, dass das größte Teil-Signal in Originalposition des größten Teil-Signals auf dem Domänenboden der verglichenen Dimensionen als erste Beobachtung des letzten Pargraphen unter der hohen Schicht-, Neuronen-, Epochenzahl aus der letzten Tabelle den Overfit von Aminis Regressionsansatz bei mehreren 3D-Signalen abbildet.  



\paragraph{Störfunktion} Für stochastische Abweichungen wird in Herr Authenrieths Kritikalitätssoftware die Klasse Noise verwendet. Sie überlagert normalverteiltes Rauschen additiv auf Punktmengen oder Gitter. Die Initialisierung erfolgt über eine Amplitude \( a > 0 \), eine Standardabweichung \( \sigma > 0 \) und optional einen Seed zur Steuerung der Pseudozufallsquelle. Es gilt $\varepsilon \sim \mathcal{N}(0, \sigma^2), \tilde{\varepsilon} = a \cdot \varepsilon$. Die Zufallsgenerierung basiert auf \texttt{numpy.default\_rng} und erlaubt sowohl Determinismus als auch Nichtdeterminismus. Entsprechend VaMai wird im Folgenden gezielt letzteres berücksichtigt.



\paragraph{Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Underfit zu annäherndem Fit und Störfunktion} 

Grundlegend ergibt die Auswertung für das einzelne Guass-Signal \textit{mit} Störfunktion sehr ähnliche Dynamik im Ergebnis des \gls{Evidenzbasierte neuronale Netze} wie im ursprünglichen Abschnitt. Ein Underfit ergibt ebenfalls eine zu flache Unsicherheitsmasse mit überkonfidentem Zuwachs zur Domänengrenze. Eine zunehmende Störfunktions-Parametrisierung mit Amplitude von 0,01 zu 1, Standardabweichung konstant 1, ergibt zwei Effekte. Das Abflachen der vom \gls{Evidenzbasierte neuronale Netze} erfassten Gesamtunsicherheitsmasse mit der Tendenz zur Fit-Dynamik. Zweitens das Verrauschen der Gesamtunsicherheitsmasse. Das Verrauschen mit Amplitude 0,2 äußert sich in fleckweisen noch flacheren Volumenabschnitten, um die größte resultierende Gesamtunsicherheitsmasse in einem Abschnitt hoher Sample-Dichte. Interpretiert man weitere Lösungsansätze für dieses Verhalten, erlaubt die Differenz der Ergebnis-Massen eine eindeutige Filterung. Dabei kann erwartet werden, dass kleine Amplituden der Störfunktion durch einen Hochpassfilter, größere Amplituden durch einen Tiefpass, sowie Kombinationen durch Bandpass auf die abgezielte Unsicherheitsmasse gefiltert werden kann. In komplexeren Fällen der Überlagerung wäre ein Ansatz wieder eine ortsweise Abtastung der Signale und additive Kombination der Lösung. 



\paragraph{Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Overfit mit Störfunktion}

Mit Störfunktion kann \textit{nicht} die selbe Overfit Dynamik wie im vorherigen Abschnitt zum einzelnen Gauss-Signal erzielt werden. Es verzögert sich \textit{nicht} lediglich durch die Epochenzahl. Bereits eine Amplitude von 0,2 sorgt bei im vorherigen Abschnitt verwendeter Epochenzahl von viertausend für gleich große um die wahre Unsicherheitsmasse äquidistant verteilte kleine Unsicherheitsmassen, in dem Fall drei, welche die Unsicherheit des Netzes ausdrücken. 



\paragraph{Multi 3D-Signal \gls{EvidentialDeepLearning} Underfit zu annäherndem Fit und Overfit mit Störfunktion} 

Wie im vorherigen Abschnitt ohne Störfunktion verzerren sich die Einzel-Signal-Beobachtungen lediglich durch die vom Netz erfasste Gesamtunsicherheitsmasse mit abweichendem Volumen von der Erwartungshaltung einer hinreichend den ursprünglichen Signalformen entsprechenden.  Ebenso erhöht sich die Unsicherheit des Netzes wieder ausgedrückt, dadurch das die aus mehreren Signalformen zusammengesetzte Unsicherheitsmasse aufgeteilt wird, auf die vom Netz äquidistant angelegten Teilvolumina um den Punkt der höchsten Signaldichte. Dies entspricht nicht der von VaMai geforderten Ähnlichkeit zu den ursprünglichen Signalformen. Außerdem erzielt es je nach Störfunktionsamplitude zu hohe oder niedrige Ergebnisse. 



\paragraph{Zwischenfazit der Durchführung - Über- und Unterkondidenz} Schwachstelle durchgeführter Experimente ist ein Über- und Unterschätzen von Grenzwerten in Abhängigkeit von der Hyperparameter-Wahl, sowie die nach oben ungebundene Loss-Funktion wie aufgezeigt im Paragraphen \glqq{}Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Overfit\grqq{}. Mittels Josangs subejctive Logic wurden erweiterte Parametrisierungen aufgezeigt, welche die Behandlung des Unterschätzens bis zur Definition von \glqq{}Zero Evidence Regions\grqq{} behandeln. Das Überschätzen ist hier, sowie in der Literatur im Fokus. Kritikalität in VaMai fordert das Einhalten der gegebener Signal-Grenzen unabdingbar ein. Ulmer et al. zeigen dafür in (28) unter (26) auf Oh und Shins Ansatz auf, die \glqq{}Negative Log Likelihood\grqq{} Loss-Funktion mit Regularisierung nach Aminis Regressionsansatz unter (27) um einen Dritt-Term zu Erweitern. Dazu wurden verantwortliche Parameter $\nu$, $\alpha$ in (26) von Oh und Shin explizit mittels partieller Differenziation gelöst, um mit (29) eine eindeutige analytische Obergrenze - unintuitiv - über eine Minimum-Funktion $U_{\nu,\alpha}=min(U_{\nu}, U_{\alpha})$ auf einem Mini-Batch aufzuzeigen (vgl. Ulmer et al. \parencite[S.17, Z. 20-25, S.18 Z. 1-8]{Ulmer2023}}). Die analytische Lösung von Oh und Shin lautet: 

\[
U_{\nu} = \frac{\beta(\nu + 1)}{\alpha \cdot \nu}\newline
U_{\alpha} = \frac{2\cdot\beta\cdot(\nu + 1)}{\nu} \dot (exp(\Phi(\alpha + \frac{1}{2}) - \Psi(\alpha)) - 1))
\]

$\alpha$, $\beta$, $\nu$ sind wie folgt abhängig. $\beta$ gilt als frei definierbarer Skalenparameter mit in der Literatur häufig gewählter Belegung: 

\[
\beta = \begin{cases}
1                                           & \text{unabhängig}, \\
\beta_{0} \cdot (\alpha - 1)                & \text{abhängig von alpha}
\end{cases}
\]

$\nu$ und $\alpha$ definieren die Loss-Begrenzung über die Fehlerwerte der Loss-Funktion mit einer lipschitz-stetigen Übergangsbedingung von quadratischer zu linearer Komponente wie im Anhang gezeigt. 

\begin{equation}
\mathcal{L}_{\mathrm{MSE}} = 
\begin{cases}
(y_{i} - \gamma)^{2})                                              & (y_{i} - \gamma)^{2} \le U_{\nu,\alpha}, \\
2\dot\sqrt{U_{\nu, \alpha}}} \cdot \left| y_{i} - \gamma \right|   & (y_{i} - \gamma)^{2} \geq U_{\nu,\alpha} \text{otherwise}
\end{cases}
&\text{Übergangsbedingung:} \\
&(y_i - \gamma)^2 = 2 \sqrt{U_{\nu,\alpha}} \cdot |y_i - \gamma| - U_{\nu,\alpha} \\
&\text{Lösung: } |y_i - \gamma| = \sqrt{U_{\nu,\alpha}}
\end{equation}

Die Min-Funktion begrenzt den quadratischen Anteil der Steigung der Loss-Funktion, um genau den Teil, in dem er stärker steigt, als die lineare Funktion. 

Herr Autenrieths Kritikalitäsraum ist mit einem oberen und unteren Schwellenwert für die Kritikalität behaftet, in den Beipsielen die zwei horizontalen Ebenen.

\[
C \colon \mathcal{X} \rightarrow [0,4, 0,6]
\]

$C$ soll die Loss-Regularisierung eingrenzen. Als Erweiterung von Oh und Shin, lässt sich eine zweite kritikalitätsabhängige Unsicherheitsgrenze einführen durch:

\[
U^*(x) := \min \left( U_\nu, U_\alpha, U_0 \cdot (1 - C(x)) \right)
\]

Dies erzwingt einen strengeren Übergang für die prädiktive Unsicherheit in hochkritischen Regionen des Eingaberaums, da erstens größere Fehler stärker bestraft werden. Zweitens, die Unsicherheit nicht skaliert werden kann, um den Fehler zu kompensieren. Drittens, epistemische und aleatorsiche Unsicherheit werden konservativer geschätzt, um den Fehler niedrig zu halten. Umgangssprachlich ließe sich das so zusammenfassen: Das Modell darf zwar seine Unsicherheit erhöhen. Aber nur, wenn der Punkt nicht kritisch ist. In sicherheitsrelevanten Regionen musst das Modell sich konservativer festlegen. Das ist ein Ansatz zur Lösung der evidentiellen Überkonfidenz, die bereits im Stand der Technik im Vergleich von \gls{Bayesianische neuronale Netze}, \gls{Evidenzbasierte neuronale Netze}, \gls{Conformal Prediction} aufgezeigt wurde. Damit kann der Ansatz gegenüber \gls{Conformal Prediction} wettbewerbsfähiger gestaltet werden.

Ein zweiter Ansatz zur expliziten Kontrolle der prädiktiven Unsicherheiten ist die Verlustfunktion um einen Schranken-Term zu ergänzen, der Varianzen außerhalb des tolerierten Kritiaklitäs-Intervalls \([0{,}4, 0{,}6]\) sanktioniert. Wiedeholt ist dabei \[
\text{Var}_{\text{gesamt}}(x) = \underbrace{\frac{\beta}{\alpha - 1}}_{\text{aleatorisch}} + \underbrace{\frac{\beta^2}{(\alpha - 1)^2(\alpha - 2)}}_{\text{epistemisch}}
\]
die Gesamtvarianz des evidentiellen Modells. Der Schranken-Term lautet:
\[
\mathcal{L}_{\text{bound}} = \lambda \cdot \left( \max(0, \text{Var}_{\text{gesamt}}(x) - 0{,}6)^2 + \max(0, 0{,}4 - \text{Var}_{\text{gesamt}}(x))^2 \right)
\]
Dieser wird zur Gesamtverlustfunktion addiert: 

\[
\mathcal{L}_{\text{total}}(x, y) = \mathcal{L}_{\text{NLL}}(x, y) + \lambda_{\text{clip}} \cdot \mathcal{L}_{\text{clip}}(x, y) + \lambda_{\text{bound}} \cdot \mathcal{L}_{\text{bound}}(x)
\]

\[
\mathcal{L}_{\text{clip}}(x, y) =
\begin{cases}
(y - \gamma)^2, & \text{falls } (y - \gamma)^2 \leq U^*(x) \\
2 \sqrt{U^*(x)} \cdot |y - \gamma| - U^*(x), & \text{sonst}
\end{cases}
\]

\[
\mathcal{L}_{\text{bound}}(x) = \left( \max(0, \text{Var}_{\text{gesamt}}(x) - 0{,}6)^2 + \max(0, 0{,}4 - \text{Var}_{\text{gesamt}}(x))^2 \right)
\]

\[
\text{Var}_{\text{gesamt}}(x) =
\begin{cases}
\frac{\beta}{\alpha - 1} + \frac{\beta^2}{(\alpha - 1)^2 (\alpha - 2)}, & \text{falls } \alpha > 2 \\
\infty, & \text{sonst}
\end{cases}
\]

Dies ist ein Ansatz sowohl die Kontrolle über die Überkonfidenz im Trainingsverhalten zu regulieren, als auch die finalen Varianzwerte an die Grenzen der Kritikalitätsfunktion zu binden.

Dies soll jetzt noch einmal unter selber Konfiguration an \glqq{}Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Overfit\grqq{} aufgezeigt werden.



\paragraph{Behandlung Überkonfidenz - \glqq{}Einzelnes 3D-Signal \gls{EvidentialDeepLearning} in Obergrenze geklippter Fit\grqq{}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO 28-07
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[!ht]
%   \centering

%   \begin{subfigure}[t]{\textwidth}
%     \centering
%     \includegraphics[width=0.6\textwidth]{../figures/y51.png}
%     \caption{Konfiguration 4, Original-Signal als Wert der Kritikalitätsfunktion \(k(\cdot)\) als \gls{GroundTruth}}
%     \label{fig:bild31}
%   \end{subfigure}

%   \vspace{0.5em}

%   \begin{subfigure}[t]{\textwidth}
%     \centering
%     \includegraphics[width=0.6\textwidth]{../figures/y52.png}
%     \caption{Konfiguration 4, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
%     \label{fig:bild32}
%   \end{subfigure}

%   \vspace{0.5em}

%   \begin{subfigure}[t]{\textwidth}
%     \centering
%     \includegraphics[width=0.6\textwidth]{../figures/y53.png}
%     \caption{Konfiguration 4, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
%     \label{fig:bild33}
%   \end{subfigure}

%   \caption{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 4}
%   \label{fig:three_subfigures3}
% \end{figure}



\paragraph{Behandlungu Unterkonfidenz und Nichtwissen} Wir definieren eine \emph{Zero Evidence Region} (ZER) als Bereich im Eingaberaum, in dem die epistemische Evidenz des Modells unterhalb eines minimalen Schwellenwertes liegt. Basierend auf Josangs Subjective Logic ergibt sich die Unsicherheitsmasse für kontinuierliche Modelle analog zur multivariaten Meinung als:

\[
u(x) := \frac{1}{\alpha(x)} \quad \text{mit Evidenz } S = \alpha(x) - 1
\]

Wir führen eine untere Evidenzschranke \( \alpha_{\min} > 0 \) ein und definieren:

\[
\text{ZER}(x) := 
\begin{cases}
1, & \text{falls } \alpha(x) < \alpha_{\min} \\
0, & \text{sonst}
\end{cases}
\]

Alternativ über die epistemische Varianz:

\[
\text{ZER}(x) := 
\begin{cases}
1, & \text{falls } \text{Var}_{\text{epist}}(x) > v_{\max} \\
0, & \text{sonst}
\end{cases}
\]

Diese Regionen signalisieren dem System explizit, dass eine verlässliche Aussage nicht getroffen werden kann. Das Modell kann \textit{grundlegend} Nichtwissen aufzeigen. 

Dies kann in die Loss-Regularierung aufgenommen werden als: 

\[
\mathcal{L}_{\text{total}}(x, y) = 
(1 - \text{ZER}(x)) \cdot \left[ 
\mathcal{L}_{\text{NLL}}(x, y) 
+ \lambda_{\text{clip}} \cdot \mathcal{L}_{\text{clip}}(x, y) 
+ \lambda_{\text{bound}} \cdot \mathcal{L}_{\text{bound}}(x) 
\right] 
+ \lambda_{\text{ZER}} \cdot \text{ZER}(x)
\]

\[
\text{mit } \quad 
\text{ZER}(x) := 
\begin{cases}
1, & \text{wenn } \alpha(x) < \alpha_{\min} \\
0, & \text{sonst}
\end{cases}
\]

\[
\mathcal{L}_{\text{clip}}(x, y) =
\begin{cases}
(y - \gamma)^2, & \text{falls } (y - \gamma)^2 \leq U^*(x) \\
2 \sqrt{U^*(x)} \cdot |y - \gamma| - U^*(x), & \text{sonst}
\end{cases}
\]

\[
\mathcal{L}_{\text{bound}}(x) = \left( 
\max(0, \text{Var}_{\text{gesamt}}(x) - 0{,}6)^2 
+ 
\max(0, 0{,}4 - \text{Var}_{\text{gesamt}}(x))^2 
\right)
\]

\[
\text{Var}_{\text{gesamt}}(x) = 
\frac{\beta}{\alpha - 1} 
+ 
\frac{\beta^2}{(\alpha - 1)^2 (\alpha - 2)}
\]

\[
U^*(x) := \min(U_\nu, U_\alpha, U_0 \cdot (1 - C(x)))
\]


Dies soll jetzt noch einmal unter selber Konfiguration an \glqq{}Einzelnes 3D-Signal \gls{EvidentialDeepLearning} Underfit\grqq{} aufgezeigt werden. Die Evidenz und \glqq{}Zero-Evidence\grqq{} kann über die diversen Parametrisierungen, die Josang liefert spezialisiert werden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO 28-07
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% \begin{figure}[!ht]
%   \centering

%   \begin{subfigure}[t]{\textwidth}
%     \centering
%     \includegraphics[width=0.6\textwidth]{../figures/y51.png}
%     \caption{Konfiguration 4, Original-Signal als Wert der Kritikalitätsfunktion \(k(\cdot)\) als \gls{GroundTruth}}
%     \label{fig:bild31}
%   \end{subfigure}

%   \vspace{0.5em}

%   \begin{subfigure}[t]{\textwidth}
%     \centering
%     \includegraphics[width=0.6\textwidth]{../figures/y52.png}
%     \caption{Konfiguration 4, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Aleatorische Unsicherheit}}
%     \label{fig:bild32}
%   \end{subfigure}

%   \vspace{0.5em}

%   \begin{subfigure}[t]{\textwidth}
%     \centering
%     \includegraphics[width=0.6\textwidth]{../figures/y53.png}
%     \caption{Konfiguration 4, vom \gls{Evidenzbasierte neuronale Netze} erkannte \gls{Epistemische Unsicherheit}}
%     \label{fig:bild33}
%   \end{subfigure}

%   \caption{Vergleich von \gls{GroundTruth}, \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} unter Konfiguration 4}
%   \label{fig:three_subfigures3}
% \end{figure}



\paragraph{Meta-Unsicherheit - Vergleich \gls{Bayesianische neuronale Netze} und \gls{Evidenzbasierte neuronale Netze} nach Josang} Zum Schluss soll Herr Mavanis Ansatz für \gls{Bayesianische neuronale Netze} mit dem der \gls{Evidenzbasierte neuronale Netze} nach Ansatz von Josangs \gls{aleatoricopinion}, \gls{epistemicopinion} für vergleichende Meta-Unsicherheit beispielhaft in Herr Autenrieths Kritikalitätsraum implementiert werden. 



\paragraph{Vorbereitung} Herr B.Sc. Mavanis \gls{Bayesianische neuronale Netze} Ansätze in Jax wurden verworfen, weil sie im Zeitrahmen zum Ende meiner Projektphase zum 24.07. zu ungenügenden Ergebnissen führten. Der HMC-Ansatz wurde nach Umstellung ebenfalls verworfen, wegen überstiegenem Budget in Trainingszeit. Es wurde in Herr Mavanis Repository unter Branch des Autoren ein Pyro Äquivalent implementiert sowie varriert mit drei, vier und fünf vollverbundenen Schichten, 32, 64, 128 Neuronen, 2000 bis im Schnitt 5000 Epochen inklusive Aufwärmprozess, 500 bis 2000 SVI Posterior Samples, Invertierung des Mittelwerts als Ausgabeneuron, Variation des BNN Modell Rangs, Unter- und Übergewichtung der Ausgabeneuronen, Übersampling der Signalspitze, Miterfassung aleatorischer Varianz, Verrauschen der epistemischen Varianz, Änderung der Aktivierungsfunktionen mittels ReLu, SiLu sowie zusätzlicher Lern-Raten-Abfall. Das beste Ergebnis wurde mit SVI, zwei vollverbundenen versteckten Schichten, 64 Neuronen, 2000 Epsioden und ohne Lern-Raten-Abfall erzielt. Das Gauss-Signal konnte auf dem Boden in der Position annähernd korrekt positioniert werden. Bei korrekter Position lieferte jede Parameter-Änderung zu sensibel-varrierende Ergebnisse in Bezug auf das Volumen, ohne auffällige Konvergenzen. In Bezug auf das Volumen konnte ein zum \gls{Evidenzbasierte neuronale Netze} vergleichbarer Underfit mit einem vergleichbar aus der Signalform austretendem wellenartigen Volumen erzielt werden. Wurde das Volumen annährend korrekt geschätzt, reagierte die Position zu sensibel auf Parameter-Änderungen. Hauptergebnis der Experimentierphase ist das zu stark mittlende Verhalten des \gls{Bayesianische neuronale Netze} über erwähnte Parameter-Variation in verwendeten Ansätzen. So wird bei großer Samplezahl am gewählten Beispiel aus Herr Autenriehts Kritikalitätsraum regelmäßig eine zur Gauss-Steigung invertierte konkave Ebene aufgespannt, wichtige gesamplete Spitzenwerte ignoriert. Die Intention die Ausgabe zu Invertieren war nicht zielführend. Zusammenfassend wurde in keinem Fall ein relevantes vergleichbares Ergebnis auf dem einfachen Gauss-Beispiel erzielt. Deshalb wird ein synthetisches Ergebnis des BNN generiert, um den Vorschlag für die Methodik aufzuzeigen und abzuschließen.  



\paragraph{Meta-Unsicherheitsvergleich zwischen ENN und synthetischem BNN} Zur Bewertung der strukturellen Unterschiede in den Unsicherheitsannahmen zwischen einem \gls{Evidenzbasierte neuronale Netze} und einem \gls{Bayesianische neuronale Netze} soll ein Divergenzmaß über die geschätzten Unsicherheitsverteilungen eingeführt werden. Da kein hinreichendes BNN-Modell vorliegt, wird die epistemische und aleatorische Unsicherheit des BNN synthetisch approximiert, indem ein Rauschterm \( \delta(x) \sim \mathcal{N}(0, \sigma^2) \) zur epistemischen ENN-Varianz addiert und anschließend in das Intervall \([0.4, 0.6]\) projiziert wird.

Für jedes \( x \in \mathcal{X} \) wird berechnet:

\[
\text{Var}_{\text{gesamt}}^{\text{ENN}}(x) = \text{Var}_{\text{epist}}^{\text{ENN}}(x) + \text{Var}_{\text{alea}}^{\text{ENN}}(x)
\]
\[
\text{Var}_{\text{gesamt}}^{\text{BNN}}(x) = \text{Var}_{\text{epist}}^{\text{ENN}}(x) + \delta(x) + \text{Var}_{\text{alea}}^{\text{ENN}}(x)
\]

Die Meta-Unsicherheit wird anschließend durch die Jensen-Shannon-Divergenz zwischen den beiden Gesamtunsicherheitsverteilungen definiert:

\[
\text{MetaUncertainty} := \text{JSD}\left( \mathcal{D}_{\text{gesamt}}^{\text{ENN}} \parallel \mathcal{D}_{\text{gesamt}}^{\text{BNN}} \right)
\]

Je höher dieser Wert, desto größer ist die Divergenz in der modellierten Unsicherheitsstruktur und damit die meta-informative Abweichung zwischen den Netzarchitekturen. 

Das definieren eines Ansatzes zur Meta-Unsicherheit erlaubt ebenso einen Ansatz zur Bewertung durch ein Framework. 

\section*{Framework zur vergleichenden Unsicherheitsanalyse der Meta-Unsicherheit}

\paragraph{1. Eingabe und Setup} Zur Analyse werden zwei Modelle betrachtet: ein \gls{Evidenzbasierte neuronale Netze} sowie ein \gls{Bayesianische neuronale Netze}, dessen Unsicherheitsverhalten synthetisch simuliert wird. wie vorher definiert. Die Berechnung erfolgt auf einem gemeinsamen Testdatensatz \( \mathcal{X}_{\text{test}} \) nach Wahl aus der eingeführten Datenpipeline. Optional kann eine Kritikalitätsfunktion \( C(x) \) eingebunden werden.

\paragraph{2. Kernmetriken je Modell}

\begin{itemize}
    \item \textbf{Epistemische Varianz:} \( \text{Var}_{\text{epist}} \)
    \item \textbf{Aleatorische Varianz:} \( \text{Var}_{\text{alea}} \)
    \item \textbf{Gesamtvarianz:} \( \text{Var}_{\text{gesamt}} = \text{Var}_{\text{epist}} + \text{Var}_{\text{alea}} \)
    \item \textbf{Accuracy:} Anteil korrekter Vorhersagen auf \( \mathcal{X}_{\text{test}} \)
\end{itemize}

\paragraph{3. Erweiterte Unsicherheitsattribute} Erweiterte Unsicherheitsattribute sind all diejenigen Bewertungsmetriken, die in der entwickelten Software Metrik-Registrierung enthalten und aus der vorherigen Rechrerche abgeleitet sind.

\paragraph{4. Bewertung und tabellarischer Vergleich} Abschließend für das Framework zur vergleichenden Unsicherheitsanalyse der Meta-Unsicherheit soll ein beispielhafter Vergleich auf einem synthethischen Ergebnis durchgeführt werden.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% TODO 28-07
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{table}[h]
\centering
\begin{tabular}{|l|c|c|c|}
\hline
\textbf{Metrik} & \textbf{ENN} & \textbf{BNN (synthetisch)} & \textbf{Differenz} \\
\hline
Epistemische Varianz (Mittel) & 0.51 & 0.49 & -0.02 \\
Aleatorische Varianz (Mittel) & 0.43 & 0.46 & +0.03 \\
JSD der Gesamtvarianz         & --   & --   & 0.21 \\
Accuracy                      & 92.1\,\% & 91.8\,\% & -0.3\,\% \\
Accuracy–Uncertainty Gap      & 0.12 & 0.08 & -0.04 \\
Kritikalitätsgewichtete Unsicherheit & 0.34 & 0.47 & +0.13 \\
\hline
\end{tabular}
\caption{Vergleich ausgewählter Unsicherheitsmetriken zwischen ENN und synthetisch approximiertem BNN}
\end{table}

\paragraph{5. Interpretation der Meta-Unsicherheit}

Anhand der ermittelten Divergenzen und Attribute kann die sogenannte \emph{Meta-Unsicherheit} definiert werden als strukturelle Abweichung zwischen den Unsicherheitsmodellen zweier Architekturen. Hohe Werte (z.\,B. JSD \( > 0.25 \)) deuten auf stark unterschiedliche Unsicherheitsannahmen hin und erfordern eine differenzierte Betrachtung der Kalibrierung und Vertrauenswürdigkeit der Vorhersagen. Die weiteren Attribute können genutzt werden, um die Kon- und Divergenzstruktur zu spezialisieren.



\paragraph{Fazit der Durchführung} Gemäß A7 sollen abschließend die wichtigsten Ansätze und Ergebnisse der Arbeit in einem Jupyter Notebook zusammenfasst werden. Dies findet sich im Anhang. 



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% in den Anhang
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\textbf{Beweis der Lipschitz-Stetigkeit von } \( \mathcal{L}(x) \):

\vspace{1em}

Definiere die Funktion als:
\[
\mathcal{L}(x) =
\begin{cases}
x^2 & \text{für } x^2 < U \\
2\sqrt{U} \cdot |x| - U & \text{für } x^2 \geq U
\end{cases}
\quad \text{mit } U := U_{\nu,\alpha}
\]

\textbf{1. Stetigkeit an der Übergangsstelle \( |x| = \sqrt{U} \):}

\[
\lim_{x \to \sqrt{U}^{-}} \mathcal{L}(x) = (\sqrt{U})^2 = U
\quad \text{und} \quad
\lim_{x \to \sqrt{U}^{+}} \mathcal{L}(x) = 2\sqrt{U} \cdot \sqrt{U} - U = 2U - U = U
\]

\(\Rightarrow \mathcal{L} \) ist stetig an \( x = \pm\sqrt{U} \)

\vspace{1em}

\textbf{2. Ableitung (Gradient) der beiden Zweige:}

\begin{itemize}
  \item Für \( |x| < \sqrt{U} \): \( \mathcal{L}'(x) = 2x \Rightarrow |\mathcal{L}'(x)| \leq 2\sqrt{U} \)
  \item Für \( |x| > \sqrt{U} \): \( \mathcal{L}'(x) = \pm 2\sqrt{U} \Rightarrow |\mathcal{L}'(x)| = 2\sqrt{U} \)
\end{itemize}

\textbf{3. Schluss:} Die Ableitung ist überall definiert oder beschränkt (außer an \( x = 0 \), wo die Funktion zwar nicht differenzierbar ist, aber trotzdem eine endliche Steigung hat), also ist \( \mathcal{L} \) global Lipschitz-stetig mit:

\[
L = \sup_{x \in \mathbb{R}} |\mathcal{L}'(x)| = 2\sqrt{U}
\]

\(\Rightarrow \mathcal{L} \) ist Lipschitz-stetig mit Konstante \( L = 2\sqrt{U} \)

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% in den Anhang ENDE
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



\end{otherlanguage}
