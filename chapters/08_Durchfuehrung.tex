% !TeX root = ../main.tex

\chapter{Durchführung}\label{chapter:durchfuehrung}



\begin{otherlanguage}{american}
  
\end{otherlanguage}



\begin{otherlanguage}{ngerman}

In der Durchführung soll \textit{A6 Implementierung und Validierung der entwickelten Unsicherheitsmetriken
und Modellierungstechniken in einer realistischen Anwendung, wie z. B. der
Unsicherheitsabschätzung in kritischen Regionen von Simulationsdaten} beantwortet werden. \parencite{amini2020deep} liefert einen bekannten validierten Ansatz aus öffentlicher Forschung. Hergert 2025 liefert einen bekannten validierten Ansatz von Unternehmensseite. 

\section*{Bekannte Ansätze}

Zur Validierung der eingesetzten Methodik für \gls{Evidenzbasierte neuronale Netze} wird der von \parencite{amini2020deep} veröffentlichte Benchmark reproduziert. Hierfür wird die im Anhang dargestellte Aufwandsschätzung berücksichtigt, die auf die im Unternehmen verfügbare Hardware abgestimmt ist, (~\ref{sec:aufwand-edl}).

\paragraph{Vergleich Amini \gls{Evidenzbasierte neuronale Netze} Regression} \parencite{amini2020deep} Sektion 4 Experiment liefert Ergebnisse für \gls{Evidenzbasierte neuronale Netze}.  
\begin{noindentquote}
\scriptsize
$
\{ \text{RMSE}, \text{NLL} \} 
\times 
\{ \text{dropout}, \text{ensembles}, \text{evidential} \} 
\times 
\{ \text{boston}, \text{concrete}, \text{energy}, \text{kin8nm}, \text{naval}, \text{power}, \text{protein}, \text{wine}, \text{yacht} \}
$  
\end{noindentquote}
Der Benchmark konnte in erster Konfiguration annähernd reproduziert werden. 
\begin{noindentquote}
  \footnotesize
  \texttt{python3 neurips2020/run\_uci\_dataset\_tests.py --datasets boston concrete energy-efficiency kin8nm naval power-plant protein wine yacht --num-trials 1 --num-epochs 5}
\end{noindentquote}
 mit Laufzeit von 9 Minuten 10.2 Sekunden $\leq$ 10 Minuten als \glqq Proof of concept\grqq. Die Konvergenz zu Aminis Werten ($\Delta (\text{RMSE}) = \left[ -0.5;\ -0.6 \right]$; $\Delta \text{NLL} = -0.08$) wird deutlich für die Konfiguration hinreichend umfangreicher Datensätze $\{\text{kin8nm}, \text{naval}, \text{protein} \}$. ~\ref{tab:comparison_ours_vs_amini_tabularx_v2} unter der Hardware-Limitierung aus Tabelle 1, Gerät 1, Hardware-Spezifikation, Anhang. % Für den visuell Abgleich siehe Abschnitt~\ref{sec:visualisierungen_edl_amini2020_regression}

\begin{table*}[!htbp]
\centering
\scriptsize
\begin{tabularx}{\textwidth}{|l|X|X|X|X|X|X|}
\hline
\multirow{}{}{\textbf{Datensatz}} 
& \multicolumn{3}{c|}{\textbf{RMSE}} 
& \multicolumn{3}{c|}{\textbf{NLL}} \\ \cline{2-7}
& \textbf{Dropout} & \textbf{Ensemble} & \textbf{Evidential} 
& \textbf{Dropout} & \textbf{Ensemble} & \textbf{Evidential} \\ \hline
boston 
& 3.678 \newline \textit{2.896} \newline \mbox{$\Delta=+0.782$} 
& 4.956 \newline \textit{2.998} \newline \mbox{$\Delta=+1.958$} 
& 3.672 \newline \textit{2.886} \newline \mbox{$\Delta=+0.786$} 
& 4.636 \newline \textit{2.755} \newline \mbox{$\Delta=+1.881$} 
& 2.981 \newline \textit{2.838} \newline \mbox{$\Delta=+0.143$} 
& 2.621 \newline \textit{2.701} \newline \mbox{$\Delta=-0.080$} \\ \hline
concrete 
& 6.905 \newline \textit{5.367} \newline \mbox{$\Delta=+1.538$} 
& 6.705 \newline \textit{5.263} \newline \mbox{$\Delta=+1.442$} 
& 6.496 \newline \textit{5.163} \newline \mbox{$\Delta=+1.333$} 
& 5.254 \newline \textit{3.213} \newline \mbox{$\Delta=+2.041$} 
& 3.271 \newline \textit{3.193} \newline \mbox{$\Delta=+0.078$} 
& 3.277 \newline \textit{3.138} \newline \mbox{$\Delta=+0.139$} \\ \hline
energy-efficiency 
& 2.721 \newline \textit{1.200} \newline \mbox{$\Delta=+1.521$} 
& 2.516 \newline \textit{0.987} \newline \mbox{$\Delta=+1.529$} 
& 2.487 \newline \textit{0.984} \newline \mbox{$\Delta=+1.503$} 
& 4.699 \newline \textit{-0.964} \newline \mbox{$\Delta=+5.663$} 
& 1.920 \newline \textit{-1.315} \newline \mbox{$\Delta=+3.235$} 
& 1.879 \newline \textit{-1.376} \newline \mbox{$\Delta=+3.255$} \\ \hline
kin8nm 
& 0.095 \newline \textit{0.096} \newline \mbox{$\Delta=-0.001$} 
& 0.084 \newline \textit{0.080} \newline \mbox{$\Delta=+0.004$} 
& 0.088 \newline \textit{0.077} \newline \mbox{$\Delta=+0.011$} 
& 1.114 \newline \textit{-1.213} \newline \mbox{$\Delta=+2.327$} 
& -1.113 \newline \textit{-1.496} \newline \mbox{$\Delta=+0.383$} 
& -0.992 \newline \textit{-1.537} \newline \mbox{$\Delta=+0.545$} \\ \hline
naval 
& 0.005 \newline \textit{0.003} \newline \mbox{$\Delta=+0.002$} 
& 0.005 \newline \textit{0.002} \newline \mbox{$\Delta=+0.003$} 
& 0.005 \newline \textit{0.002} \newline \mbox{$\Delta=+0.003$} 
& -2.438 \newline \textit{-6.233} \newline \mbox{$\Delta=+3.795$} 
& -3.931 \newline \textit{-6.967} \newline \mbox{$\Delta=+3.036$} 
& -3.747 \newline \textit{-6.987} \newline \mbox{$\Delta=+3.240$} \\ \hline
power-plant 
& 3.349 \newline \textit{4.049} \newline \mbox{$\Delta=-0.700$} 
& 3.467 \newline \textit{4.021} \newline \mbox{$\Delta=-0.554$} 
& 3.377 \newline \textit{3.987} \newline \mbox{$\Delta=-0.610$} 
& 5.281 \newline \textit{2.795} \newline \mbox{$\Delta=+2.486$} 
& 2.678 \newline \textit{2.765} \newline \mbox{$\Delta=-0.087$} 
& 2.671 \newline \textit{2.755} \newline \mbox{$\Delta=-0.084$} \\ \hline
protein 
& 4.355 \newline \textit{4.512} \newline \mbox{$\Delta=-0.157$} 
& 3.939 \newline \textit{4.480} \newline \mbox{$\Delta=-0.541$} 
& 4.169 \newline \textit{4.430} \newline \mbox{$\Delta=-0.261$} 
& 4.266 \newline \textit{2.871} \newline \mbox{$\Delta=+1.395$} 
& 2.742 \newline \textit{2.837} \newline \mbox{$\Delta=-0.095$} 
& 2.706 \newline \textit{2.779} \newline \mbox{$\Delta=-0.073$} \\ \hline
wine 
& 0.826 \newline \textit{0.648} \newline \mbox{$\Delta=+0.178$} 
& 0.829 \newline \textit{0.631} \newline \mbox{$\Delta=+0.198$} 
& 0.722 \newline \textit{0.609} \newline \mbox{$\Delta=+0.113$} 
& 2.255 \newline \textit{-0.574} \newline \mbox{$\Delta=+2.829$} 
& 1.390 \newline \textit{-0.618} \newline \mbox{$\Delta=+2.008$} 
& 1.187 \newline \textit{-0.679} \newline \mbox{$\Delta=+1.866$} \\ \hline
yacht 
& 7.046 \newline \textit{1.711} \newline \mbox{$\Delta=+5.335$} 
& 9.197 \newline \textit{1.337} \newline \mbox{$\Delta=+7.860$} 
& 5.557 \newline \textit{1.252} \newline \mbox{$\Delta=+4.305$} 
& 5.170 \newline \textit{-1.637} \newline \mbox{$\Delta=+6.807$} 
& 3.041 \newline \textit{-2.257} \newline \mbox{$\Delta=+5.298$} 
& 2.401 \newline \textit{-2.341} \newline \mbox{$\Delta=+4.742$} \\ \hline
\end{tabularx}
\caption{Vergleich reproduzierte Ergebnisse mit Benchmark \textit{Amini et al. (2020)}}
\label{tab:comparison_ours_vs_amini_tabularx_v3}
\end{table*}


\paragraph{Vergleich Amini \gls{Evidenzbasierte neuronale Netze} Klassifikation} \parencite{amini2020deep} \texttt{gen\_depth\_results.py} liefert Ergebnisse für \gls{Evidenzbasierte neuronale Netze} für die Aufgabe \glqq Monocular depth estimation\grqq. Der Benchmark konnte reproduziert werden mit einer Konfiguration $N_\text{batch}=25$, $N_\text{adv}=3$ unter der Hardware-Limitierung aus Tabelle 1, Gerät 1, Hardware-Spezifikation, Anhang. % Für den visuell Abgleich siehe Abschnitt~\ref{sec:visualisierungen_edl_amini2020_classification}



\section*{Eigenentwicklung}

Benötigt wurde eine Datenpipeline. In Entwicklung mit Herr M.Sc. N. Mavani sollte für den Vergleich von \gls{Bayesianische neuronale Netze}, \gls{Evidenzbasierte neuronale Netze} als Baseline von Mavanis Seite auf \parencite{Depeweg2019}, von meiner Seite auf \parencite{Ulmer2023} aufgesetzt werden.\newline

Dazu wurden von \parencite{Depeweg2019} verwendete Datensätze verwendet und ergänzt um in \hyperref[sec:datensaetze]{Datensätze} gelistete. Aufgesetzte Stufen der Datenpipeline sind eine Vorverarbeitung zum Umwandeln von Formaten, Einlesen in Torch Dataset Klassen, Torch Trainingsklassen, Visualisierungsklassen.\newline

Aus den Datensätzen wurden Trainings und Visualisierungen abgeleitet und generifiziert. Ebenso wurden recherchierte Loss-Funktionen generifiziert. Trainings wurden auf einen modernen Standard ergänzt wie Checkpoints, Patience-Epochen, vergleichbar und generifiziert. Nach Generifizierung bleiben einer Klasse überwiegend nur noch konfigurierbare Anteile vorbehalten. Eine Konfiguration wurde als Refactor geplant und soll von dort aus zu einer zentralen Konfiguration hochgereicht werden.\newline

Im Projekt behandelte Metriken wurden aufgefasst mit einer Klasse Metrik, Metrik-Thresholds, Metrik-Registry, Metrik-Registry-Defintions deren Name die Funktion hinreichend vorgibt.\newline

Integriert wurden Lösungen von Herr M.Sc. Mavani der sich zur \gls{Bayesianische neuronale Netze} Auswertung für ein Python Pyro Jax Backend entschieden und damit Lösungen nach den Pyroworkflows für Hamilton Monte Carlo (\gls{hmc}), Stochastic Variational Inference (\gls{svi}) geliefert hat.


% \subsection*{Experiment}

\begin{table}[htbp]
\centering
\begin{tabularx}{\textwidth}{|l|l|l|l|l|l|l|X|}
\hline
 & \texttt{nll} & \texttt{abs} & \texttt{mse} & \texttt{kl} & \texttt{scaled} & \texttt{variational} & \texttt{full} \\
\hline
EDNN Accuracy & & & & & & & \\
\hline
BNN Accuracy & & & & & & & \\
\hline
\end{tabularx}
\caption{Zusammenfassung der Evidential Loss Varianten}
\end{table}


\section*{Visueller Abgleich \gls{Bayesianische neuronale Netze}-\gls{Evidenzbasierte neuronale Netze}}

\begin{figure}[!h]
  \centering
  \begin{minipage}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../figures/bnn_posterior_predictive.png}
      \caption{Ninand Mavani Visual Benchmark \gls{Bayesianische neuronale Netze} \gls{hmc}}
  \end{minipage}%
  \begin{minipage}{0.5\textwidth}
      \centering
      \includegraphics[width=\textwidth]{../figures/bnn_svi_predictive.png}
      \caption{Ninand Mavani Visual Benchmark \gls{Bayesianische neuronale Netze} \gls{svi}}
  \end{minipage}
\end{figure}


% \subsection*{Schnittstelle}

% \subsection*{Benchmark EDNN}

% \subsection*{Benchmark BNN-EDNN}

% \begin{table}[h!]
% \centering
% \resizebox{\textwidth}{!}{
% \begin{tabular}{|c|c|c|c|c|}
% \hline
% \textbf{Sigma (Standard Deviation)} & \textbf{BNN Variance} & \textbf{BNN Accuracy (\%)} & \textbf{EDNN Variance} & \textbf{EDNN Accuracy (\%)} \\
% \hline
% 0 & 0.0 & 0 & 0.0 & 0 \\
% \hline
% \end{tabular}
% }
% \caption{Vergleich von BNN und EDNN mit Meta-Metrik im 2-Sigma-Bereich}
% \label{tab:BNN_EDNN_Comparison}
% \end{table}


\subsection*{VaMai - Kritikalitätsraum - Validierung - Annahmen}

Vamai verfolgt mit dem Kritikalitätsraum die Idee einer Kritikalitätsabdeckung. Umgangssprachlich möchte man den Raum so gut abdecken, dass mit einer gewissen Wahrscheinlichkeit kein Kritikalitätsbereich mehr in diesem Raum vorhanden ist. Als Beweisidee äußert sich das wie folgt: Zeige, dass durch die gewählte Methode (z.\,B. Evidential Deep Learning und Unsicherheitsaggregation) die Wahrscheinlichkeit dafür, dass ein unentdeckter Kritikalitätsraum existiert, unter einem Schwellenwert liegt.

\[ 
\mathbb{P}\bigl(\exists\, x \in \Omega \setminus \widehat{\Omega}_k : \text{Kritikalität}(x) > \tau \bigr) < \varepsilon. 
\]

Das heißt, die Wahrscheinlichkeit, dass irgendwo im Raum $\Omega$ noch ein Punkt existiert, dessen Kritikalität den Schwellenwert überschreitet, aber nicht durch diese Methode erkannt wurde, ist hinreichend klein (z.\,B. unter 5\,\% mit $\mu \pm 2\sigma$). $\Omega \subset \mathbb{R}^{n}$ ist der gesamte Zustands- oder Szenarienraum. $\Omega_k := \left\{ x \in \Omega \;\big|\; k(x) \ge \tau \right\}$ ist der echte Kritikalitätsraum. $\widehat{\Omega}_k := \left\{ x \in \Omega \;\big|\; k(x) \ge \tau \right\}$ ist der durch die Methode erkannte Kritikalitätsraum. $\varepsilon \in (0,1)$ ist das akzeptierte Fehlerniveau.

Zu beweisen ist: $\mathbb{P}\bigl( \Omega_k \setminus \widehat{\Omega}_k \neq \emptyset \bigr) \le \varepsilon.$ Dazu wird folgende Vorgehensweise in Betracht gezogen: 

% enger formalisierbar, siehe Notizen

\begin{enumerate}
  \item Unsicherheitsmodellierung mittels Evidential Deep Learning
  \begin{itemize}
    \item Liefert für jeden Punkt $x \in \Omega$ eine Wahrscheinlichkeitsverteilung mit Varianz $\mathrm{Var}(x) = \sigma^{2}_{\text{epistemisch}} + \sigma^{2}_{\text{aleatorisch}}$.
    \item Kritikalität ist Funktion der Größe $k(x)$.
  \end{itemize}
  \item Sampling- / Klassifikationsabdeckung
  \begin{itemize}
    \item Raum $\Omega$ wurde mit Dichte $\delta$-gesampled (zum Beispiel mittels Gitter, Trajektorien, Pfaden) von VaMai Projektteilnehmer B.Sc. Pascal Imholze mit dem Ziel einer möglichst hohen Abdeckung.
    \item Wenn $\forall x \in \Omgea$ ein Sample $x' \in \widehat{\Omega}_k$ mit $\lVert x - x' \rVert \le \delta'$ existiert, gilt \glqq dichte Abdeckung \grqq.
  \end{itemize}
  \item Uniforme Konvergenz / PAC-ähnliche Argumentation
  \begin{itemize}
    \item Zeige, dass mit Vorhersagewahrscheinlichkeit, Unsicherheitsabschätzung mittels Methodik mit zunehmender Samplezahl $N$ gegen den wahren Kritikalitätsraum konvergiert. 
    \[
    \mathbb{P}\bigl( \sup_{x \in \Omega} \bigl\lvert f(x) - \widehat{k}(x) \bigr\rvert \ge \eta \bigr) \le \varepsilon.
    \]
    \item Wenn $\widehat{k(x)} \approx k(x)$ auf ganz $\Omega$, gilt auch $\widehat{\Omega}_{k} \approx \Omega_{k}$. Es folgt, dass die Wahrscheinlichkeit einen echten Kritikalitätsraum \textit{nicht} zu erkennen kleiner als $\epsilon$ ist.
  \end{itemize}

\end{enumerate}

\subsection*{VaMai - Kritikalitätsraum - Validierung - Experiment}

Herr Autenrieths Kritikalitätsraum-Software wurden eingebunden in die bisherige Eigenentwicklung. Die Umgebung liefert einen Raum in dem beispielhafte Signalumgebungen 
% evtl. formalisieren.
zusammengesetzt werden können. Er ist bereits grundlegend unsicherheitsbehaftbar. Zuerst wurde eine Visualisierung nach Amini Deep Evidential Regression 2020 ergänzt. Als nächstes wurde ein \gls{EvidentialDeepLearning}-Training unter Aminis vorhandenen bereits optimierten Modellen (siehe Rekonstruktion Aminis Ergebnisse) in den Hauptablauf integriert. 

Die synthetischen Daten ersetzen in diesem Fall reale Messungen, Simulationsergebnisse $\{ x_{i}, y_{i} \}$. $x_{i}$ ist ein Eingabepunkt der als kleinste Einheit bereits als Szenario im Parameterraum zählen kann. $y_{i}$ liefert die Kritikalität in diesem Punkt. Das \gls{EvidentialDeepLearning}-Modell beantwortet die Frage, mit welcher Unsicherheit es glaube, dass $y$ an einem neuen Punkt läge. 

\begin{figure}[!ht]
  \centering
  \includegraphics[width=\textwidth]{../figures/x4.png}
  \caption{R4.4 Angen. Grund-Verteilung auf unbek. Daten für \gls{EvidentialDeepLearning}-Modell n. Amini 2020}
\end{figure}

Das evidentielle Modell nach Amini ordnet das als $y \sim \mathcal{N}(\mu, \sigma^2)$ zu. $\mu$, $\sigma^2$ kennt das Modell nicht aus Vorwissen. Es nimmt stattdessen an, erstens $\mu$ ist unsicherheitsbehaftet, damit folgt $\mu \sim \mathcal{N}(\gamma, \sigma^{2} / \lambda)$. Zweitens $\sigma^{2}$ ist unsicherheitsbehaftet, damit folgt $\sigma^{2} \sim \mathcal{NIG}(\alpha, \beta)$. So sucht das Modell $p(y|\gamma,\lambda,\alpha,\beta)$. Die Daten liefern die Evidenz als Ergebnis. Das \gls{EvidentialDeepLearning}-Modell schätzt direkt die Hyperparameter ausgehend von den Eingaben $x$, $f(x) \rightarrow \{ \gamma(x), \lambda(x), \alpha(x), \beta(x) \}$. Es ergibt sich für jeden Punkt $x$ eine bedingte Unsicherheitsverteilung für $y$. Amini verdeutlicht dies kompakt in Gleichungen (3), (4), (5), Abbildung R.4.4 (vgl. Amini 2020 \parencite{amini2020}). 

Ergebende Unsicherheitsmasse ist Student-t-verteilt (vgl. Gleichungen (6), (7)), damit ist Mittelwert, Skalierung, Freiheitsgrad bekannt. Wie in R4.4 visualisiert die Zusammensetzung der Student-t-Verteilung, in \glqq niedriger \grqq und \glqq höherer \grqq Ordnung. Erste vom Modell geschätze Wahrscheinlichkeitsmasse um einen Schätzwert ergibt sich aus Gleichung 4 ohne Regularisierung in R4.4. platikurtisch hellblau. Regularisierungsbedingungen formen die Verteilung unter Evidenz-Kriterien leptokurtisch dunkelblau. Machine-Learning-basiert regularisieren Loss-Funktion nach dem Min-Max-Prinzip angenommene Fehler- gegen Zielgrößen, hier durch Amini beschränkt Varianz gegenüber Evidenz (vgl. Gleichung (8), (9), (10) unter (5)). 

% Herr Autenrieths Kritikalitätsraums Unsicherheitsbehaftung wird zunächst äquivalent zu Aminis Modell R.4.4 refaktorisiert. Noise-Klasse eingebundenl, Determinismus für Zufallsgeneration überschrieben. 

% Ergebnisbilder.



\section{}

Abschließend sollen nach A7 die wichtigsten Ansätze und Ergebnisse der Arbeit in einem kurzen Jupyter Notebook zusammenfasst werden. 



\end{otherlanguage}
