% !TeX root = ../main.tex

\chapter{Problemstellung}
\label{chapter:problemstellung}

Die zentrale Problemstellung dieser Arbeit liegt in der Bestimmung und Analyse der \textbf{aleatorischen} und \textbf{epistemischen Gesamtunsicherheit} innerhalb eines sogenannten \emph{Kritikalitätsraums}, welcher durch die Verletzung sicherheitsrelevanter Grenzen in einem machine-learning-basierten Surrogatmodell definiert ist.

In komplexen Anwendungsszenarien – insbesondere bei der simulationsgestützten Validierung autonomer Systeme – stellt der Kritikalitätsraum jene Teilmenge des gesamten Szenarienraums dar, in der das Systemverhalten als potenziell kritisch oder risikobehaftet einzustufen ist.

Dabei gilt es, zwei unterschiedliche Unsicherheitsquellen präzise zu erfassen und voneinander abzugrenzen:

\begin{itemize}
  \item \textbf{Aleatorische Unsicherheit}, verursacht durch stochastische Variabilität in den Eingabedaten,
  \item \textbf{Epistemische Unsicherheit}, bedingt durch strukturelle Modellunsicherheit und fehlendes Wissen.
\end{itemize}

Die zentrale Herausforderung besteht darin, diese beiden Unsicherheitsarten über den Kritikalitätsraum hinweg zu \emph{aggregieren}, sodass eine verlässliche Gesamtunsicherheitsmasse entsteht. Diese aggregierte Größe soll sowohl als Kriterium für die \textbf{Vertrauenswürdigkeit} eines Modells als auch für die \textbf{Relevanz} bestimmter Szenarien dienen können.

Klassische Verfahren zur Unsicherheitsquantifizierung stoßen hierbei an ihre methodischen Grenzen. Entweder modellieren sie nur eine der beiden Unsicherheitsarten oder sie berücksichtigen keine explizite räumliche Abgrenzung wie sie durch den Kritikalitätsraum gegeben ist.

Ziel dieser Arbeit ist es daher, bestehende Limitierungen zu überwinden und ein methodisches Fundament für die aggregierte Unsicherheitsanalyse in sicherheitskritischen ML-Anwendungen zu schaffen.
