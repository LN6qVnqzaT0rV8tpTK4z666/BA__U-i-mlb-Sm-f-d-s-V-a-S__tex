% !TeX root = ../main.tex

\chapter{Diskussion}\label{chapter:diskussion}



% \begin{otherlanguage}{american}
% Following the quantitative evaluation of the trained models, this chapter presents a reflective discussion of the results with regard to the previously defined research questions. The focus is on the significance of the modeled uncertainties, the calibration, and the behavior of the EDN in comparison to established baselines from the literature. The results are analyzed in terms of their technical relevance for scenario-based validation, robustness against critical input ranges, and interpretability in the context of safety-relevant decision-making processes. In addition, potential limitations, observations of deviating model behavior, and open methodological questions are critically classified.

% \section{Assessment of trustworthiness}

% %  % Can you trust your ML metric? 
% %  % Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

% Jürgens et al. (2024) and Herd et al. (2024) provide the central point of contention for the work by critically questioning the uncertainty in machine learning models and their metrics. Jürgens et al. note that evidential deep learning has difficulty reliably representing epistemic uncertainty, especially with regard to variance. They argue that uncertainty estimates vary greatly across different training runs due to the instability of model parameters, which points to identifiability problems. This leads to unreliable quantifications of uncertainty, especially in internal loss minimization. On the other hand, Herd et al. propose supplementing traditional performance metrics such as accuracy and precision with subjective logic to systematically address uncertainties and thus better evaluate actual model performance. They emphasize that common metrics often fail to account for uncertainties, resulting in overestimation of model performance. Both papers make it clear that uncertainty in models is often not correctly captured and that a probabilistic treatment of uncertainty is necessary to evaluate actual model performance more realistically ~\parencite{Jurgens.} ~\parencite{Herd04082024}.

% \section{Evaluation of meta-reliability (meta-uncertainty)}

% The broader term “meta-uncertainty” is not standardized in scientific literature. One goal of this paper is to go beyond the classic separation of AC and EC and develop a higher-level measure of the reliability of model-based predictions for the evidence-based and Bayesian approaches—a so-called meta-uncertainty. This refers to the systematic aggregation, interpretation, and evaluation of the uncertainties generated by the model itself, with the aim of deriving a global, context-sensitive measure of confidence. 

% Meta-uncertainty is not a direct model output, but rather a function of several uncertainty components that are considered cumulatively along a trajectory, a scenario, or within the framework of an entire validation run. Typical influencing factors are:
% \begin{itemize}
%   \item the local evidence (\(\alpha^{-1}\)) of the model in combination with the prediction quality (\(\left| y - \mu \right|\)),
%   \item the distribution of epistemic variance over the input space,
%   \item structural features such as clusters of uncertain areas or abrupt jumps in evidence (e.g., along a path),
%   \item as well as metrics such as the Jensen-Shannon divergence between successive uncertainty distributions or models.
% \end{itemize}

% To capture meta-uncertainty, an aggregated risk measure can be defined that represents, for example, a weighted sum of local uncertainties, variance gradients, or uncalibrability. In practice, such a measure provides an assessment of how consistent, robust, and explainable a model's predictions are in the context of the task at hand.

% Especially in safety-critical scenarios—such as in the validation of autonomous systems or simulation-based decision support—meta-uncertainty makes a decisive contribution to confidence assessment. It supplements classic metrics of model quality with an interpretable, quantitative statement about the reliability of the model itself—not just its output.

% % Take up results in a scientifically meaningful way.
% \end{otherlanguage}



\begin{otherlanguage}{ngerman}
Nach der quantitativen Auswertung der trainierten Modelle folgt in diesem Kapitel eine reflektierende Diskussion der Ergebnisse im Hinblick auf die zuvor definierten Forschungsfragen. Dabei steht die Aussagekraft der modellierten Unsicherheiten, die Kalibrierung sowie das Verhalten des EDN im Vergleich zu etablierten Baselines aus der Literatur im Mittelpunkt. Die Ergebnisse werden hinsichtlich ihrer technischen Relevanz für szenariobasierte Validierung, Robustheit gegenüber kritischen Eingabebereichen sowie ihrer Interpretierbarkeit im Kontext sicherheitsrelevanter Entscheidungsprozesse analysiert. Darüber hinaus werden potenzielle Einschränkungen, Beobachtungen abweichender Modellverhalten sowie offene methodische Fragen kritisch eingeordnet.

\section{Bewertung der Vertrauenswürdigkeit}

%  % Can you trust your ML metric? 
%  % Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

Den zentralen Gegenstandpunkt zur Arbeit liefern Jürgens et al. (2024), Herd et al. (2024) durch kritisches Hinterfragen der Unsicherheit in \gls{machinelearning}-Modellen und deren Kennzahlen. Jürgens et al. stellen fest, dass \gls{EvidentialDeepLearning} Schwierigkeiten hat, epistemische Unsicherheit zuverlässig darzustellen, insbesondere in Bezug auf die Varianz. Sie argumentieren, dass die Unsicherheitsschätzungen aufgrund der Instabilität der Modellparameter über verschiedene Trainingsdurchläufe hinweg stark variieren, was auf Identifizierbarkeitsprobleme hindeutet. Dies führt zu unzuverlässigen Quantifizierungen der Unsicherheit, insbesondere in der inneren Verlustminimierung. Auf der anderen Seite schlagen Herd et al. vor, traditionelle Leistungskennzahlen wie Genauigkeit und Präzision durch subjektive Logik zu ergänzen, um Unsicherheiten systematisch zu behandeln und so die tatsächliche Modellleistung besser zu bewerten. Sie betonen, dass die gängigen Metriken oft die Unsicherheiten nicht berücksichtigen, wodurch die Modellleistung überschätzt wird. Beide Arbeiten machen deutlich, dass die Unsicherheit in den Modellen häufig nicht korrekt erfasst wird und dass eine probabilistische Behandlung der Unsicherheit notwendig ist, um die tatsächliche Modellleistung realistischer zu bewerten ~\parencite{Jurgens.} ~\parencite{Herd04082024}.

\section{Bewertung der Meta-Vertrauenswürdigkeit (Meta-Unsicherheit)}

Der weiterführende Begriff „Meta-Unsicherheit“ ist in der wissenschaftlichen Literatur nicht standardisiert etabliert. Ein Ziel der vorliegenden Arbeit besteht darin, über die klassische Trennung von AC und EC hinaus ein übergeordnetes Maß für die Vertrauenswürdigkeit modellbasierter Vorhersagen für den evidenzbasierten und bayesischen Ansatz zu entwickeln – eine sogenannte Meta-Unsicherheit. Darunter wird die systematische Aggregation, Interpretation und Bewertung der vom Modell selbst generierten Unsicherheiten verstanden, mit dem Ziel, ein globales, kontextsensitives Vertrauensmaß abzuleiten. 

Die Meta-Unsicherheit ergibt sich nicht als direkter Modelloutput, sondern als Funktion aus mehreren Unsicherheitskomponenten, die entlang einer Trajektorie, eines Szenarios oder im Rahmen eines gesamten Validierungslaufs kumulativ betrachtet werden. Typische Einflussgrößen sind dabei:
\begin{itemize}
  \item die lokale Evidenz (\(\alpha^{-1}\)) des Modells in Kombination mit der Vorhersagegüte (\(\left| y - \mu \right|\)),
  \item die Verteilung epistemischer Varianz über den Eingaberaum,
  \item strukturelle Merkmale wie Häufung unsicherer Bereiche oder abrupte Evidenzsprünge (z.\,B. entlang eines Pfades),
  \item sowie Metriken wie die Jensen-Shannon-Divergenz zwischen aufeinanderfolgenden Unsicherheitsverteilungen oder Modellen.
\end{itemize}

Zur Erfassung der Meta-Unsicherheit kann ein aggregiertes Risikomaß definiert werden, das z.\,B. eine gewichtete Summe lokaler Unsicherheiten, Varianzgradienten oder Nichtkalibrierbarkeit darstellt. In der Praxis liefert ein solches Maß eine Einschätzung darüber, wie konsistent, robust und erklärbar die Vorhersagen eines Modells im Kontext der gestellten Aufgabe sind.

Gerade in sicherheitskritischen Szenarien – etwa in der Validierung autonomer Systeme oder der simulationsbasierten Entscheidungsunterstützung – liefert Meta-Unsicherheit einen entscheidenden Beitrag zur Vertrauensbewertung. Sie ergänzt klassische Metriken der Modellgüte durch eine interpretierbare, quantitative Aussage über die Zuverlässigkeit des Modells selbst – nicht nur dessen Ausgabe.

% Ergebnisse wissenschaftlich sinnvoll aufgreifen.
\end{otherlanguage}