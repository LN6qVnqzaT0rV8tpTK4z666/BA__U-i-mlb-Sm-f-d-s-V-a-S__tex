% !TeX root = ../main.tex

\chapter{Diskussion}\label{chapter:diskussion}



% \begin{otherlanguage}{american}
% Following the quantitative evaluation of the trained models, this chapter presents a reflective discussion of the results with regard to the previously defined research questions. The focus is on the significance of the modeled uncertainties, the calibration, and the behavior of the EDN in comparison to established baselines from the literature. The results are analyzed in terms of their technical relevance for scenario-based validation, robustness against critical input ranges, and interpretability in the context of safety-relevant decision-making processes. In addition, potential limitations, observations of deviating model behavior, and open methodological questions are critically classified.

% \section{Assessment of trustworthiness}

% %  % Can you trust your ML metric? 
% %  % Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

% Jürgens et al. (2024) and Herd et al. (2024) provide the central point of contention for the work by critically questioning the uncertainty in machine learning models and their metrics. Jürgens et al. note that evidential deep learning has difficulty reliably representing epistemic uncertainty, especially with regard to variance. They argue that uncertainty estimates vary greatly across different training runs due to the instability of model parameters, which points to identifiability problems. This leads to unreliable quantifications of uncertainty, especially in internal loss minimization. On the other hand, Herd et al. propose supplementing traditional performance metrics such as accuracy and precision with subjective logic to systematically address uncertainties and thus better evaluate actual model performance. They emphasize that common metrics often fail to account for uncertainties, resulting in overestimation of model performance. Both papers make it clear that uncertainty in models is often not correctly captured and that a probabilistic treatment of uncertainty is necessary to evaluate actual model performance more realistically ~\parencite{Jurgens.} ~\parencite{Herd04082024}.

% \section{Evaluation of meta-reliability (meta-uncertainty)}

% The broader term “meta-uncertainty” is not standardized in scientific literature. One goal of this paper is to go beyond the classic separation of AC and EC and develop a higher-level measure of the reliability of model-based predictions for the evidence-based and Bayesian approaches—a so-called meta-uncertainty. This refers to the systematic aggregation, interpretation, and evaluation of the uncertainties generated by the model itself, with the aim of deriving a global, context-sensitive measure of confidence. 

% Meta-uncertainty is not a direct model output, but rather a function of several uncertainty components that are considered cumulatively along a trajectory, a scenario, or within the framework of an entire validation run. Typical influencing factors are:
% \begin{itemize}
%   \item the local evidence (\(\alpha^{-1}\)) of the model in combination with the prediction quality (\(\left| y - \mu \right|\)),
%   \item the distribution of epistemic variance over the input space,
%   \item structural features such as clusters of uncertain areas or abrupt jumps in evidence (e.g., along a path),
%   \item as well as metrics such as the Jensen-Shannon divergence between successive uncertainty distributions or models.
% \end{itemize}

% To capture meta-uncertainty, an aggregated risk measure can be defined that represents, for example, a weighted sum of local uncertainties, variance gradients, or uncalibrability. In practice, such a measure provides an assessment of how consistent, robust, and explainable a model's predictions are in the context of the task at hand.

% Especially in safety-critical scenarios—such as in the validation of autonomous systems or simulation-based decision support—meta-uncertainty makes a decisive contribution to confidence assessment. It supplements classic metrics of model quality with an interpretable, quantitative statement about the reliability of the model itself—not just its output.

% % Take up results in a scientifically meaningful way.
% \end{otherlanguage}



\begin{otherlanguage}{ngerman}
% Nach der quantitativen Auswertung der trainierten Modelle folgt in diesem Kapitel eine reflektierende Diskussion der Ergebnisse im Hinblick auf die zuvor definierten Forschungsfragen. Dabei steht die Aussagekraft der modellierten Unsicherheiten, die Kalibrierung sowie das Verhalten des EDN im Vergleich zu etablierten Baselines aus der Literatur im Mittelpunkt. Die Ergebnisse werden hinsichtlich ihrer technischen Relevanz für szenariobasierte Validierung, Robustheit gegenüber kritischen Eingabebereichen sowie ihrer Interpretierbarkeit im Kontext sicherheitsrelevanter Entscheidungsprozesse analysiert. Darüber hinaus werden potenzielle Einschränkungen, Beobachtungen abweichender Modellverhalten sowie offene methodische Fragen kritisch eingeordnet.

% \paragraph{Bewertung der Vertrauenswürdigkeit}

% %  % Can you trust your ML metric? 
% %  % Is Epistemic Uncertainty Faithfully Represented by Evidential Deep Learning Methods?

% Den zentralen Gegenstandpunkt zur Arbeit liefern Jürgens et al. (2024), Herd et al. (2024) durch kritisches Hinterfragen der Unsicherheit in \gls{machinelearning}-Modellen und deren Kennzahlen. Jürgens et al. stellen fest, dass \gls{EvidentialDeepLearning} Schwierigkeiten hat, epistemische Unsicherheit zuverlässig darzustellen, insbesondere in Bezug auf die Varianz. Sie argumentieren, dass die Unsicherheitsschätzungen aufgrund der Instabilität der Modellparameter über verschiedene Trainingsdurchläufe hinweg stark variieren, was auf Identifizierbarkeitsprobleme hindeutet. Dies führt zu unzuverlässigen Quantifizierungen der Unsicherheit, insbesondere in der inneren Verlustminimierung. Auf der anderen Seite schlagen Herd et al. vor, traditionelle Leistungskennzahlen wie Genauigkeit und Präzision durch subjektive Logik zu ergänzen, um Unsicherheiten systematisch zu behandeln und so die tatsächliche Modellleistung besser zu bewerten. Sie betonen, dass die gängigen Metriken oft die Unsicherheiten nicht berücksichtigen, wodurch die Modellleistung überschätzt wird. Beide Arbeiten machen deutlich, dass die Unsicherheit in den Modellen häufig nicht korrekt erfasst wird und dass eine probabilistische Behandlung der Unsicherheit notwendig ist, um die tatsächliche Modellleistung realistischer zu bewerten ~\parencite{Jurgens.} ~\parencite{Herd04082024}.

% \paragraph{Bewertung der Meta-Vertrauenswürdigkeit (Meta-Unsicherheit)}

% Der weiterführende Begriff „Meta-Unsicherheit“ ist in der wissenschaftlichen Literatur nicht standardisiert etabliert. Ein Ziel der vorliegenden Arbeit besteht darin, über die klassische Trennung von AC und EC hinaus ein übergeordnetes Maß für die Vertrauenswürdigkeit modellbasierter Vorhersagen für den evidenzbasierten und bayesischen Ansatz zu entwickeln – eine sogenannte Meta-Unsicherheit. Darunter wird die systematische Aggregation, Interpretation und Bewertung der vom Modell selbst generierten Unsicherheiten verstanden, mit dem Ziel, ein globales, kontextsensitives Vertrauensmaß abzuleiten. 

% Die Meta-Unsicherheit ergibt sich nicht als direkter Modelloutput, sondern als Funktion aus mehreren Unsicherheitskomponenten, die entlang einer Trajektorie, eines Szenarios oder im Rahmen eines gesamten Validierungslaufs kumulativ betrachtet werden. Typische Einflussgrößen sind dabei:
% \begin{itemize}
%   \item die lokale Evidenz (\(\alpha^{-1}\)) des Modells in Kombination mit der Vorhersagegüte (\(\left| y - \mu \right|\)),
%   \item die Verteilung epistemischer Varianz über den Eingaberaum,
%   \item strukturelle Merkmale wie Häufung unsicherer Bereiche oder abrupte Evidenzsprünge (z.\,B. entlang eines Pfades),
%   \item sowie Metriken wie die Jensen-Shannon-Divergenz zwischen aufeinanderfolgenden Unsicherheitsverteilungen oder Modellen.
% \end{itemize}

% Zur Erfassung der Meta-Unsicherheit kann ein aggregiertes Risikomaß definiert werden, das z.\,B. eine gewichtete Summe lokaler Unsicherheiten, Varianzgradienten oder Nichtkalibrierbarkeit darstellt. In der Praxis liefert ein solches Maß eine Einschätzung darüber, wie konsistent, robust und erklärbar die Vorhersagen eines Modells im Kontext der gestellten Aufgabe sind.

% Gerade in sicherheitskritischen Szenarien – etwa in der Validierung autonomer Systeme oder der simulationsbasierten Entscheidungsunterstützung – liefert Meta-Unsicherheit einen entscheidenden Beitrag zur Vertrauensbewertung. Sie ergänzt klassische Metriken der Modellgüte durch eine interpretierbare, quantitative Aussage über die Zuverlässigkeit des Modells selbst – nicht nur dessen Ausgabe.

% % Ergebnisse wissenschaftlich sinnvoll aufgreifen.




\paragraph{Diskussion} 

\newline

\textbf{R1} Die Resultate weisen darauf hin, dass Bayesische Neuronale Netze bei bestimmten Signalprofilen und unter stark streuendem Rauschen präzisere Approximationen erzielen können. Evidentielle Neuronale Netze ergänzen diese Eigenschaft durch eine höhere Sensitivität für selten gesampelte, ausgeprägte Signalamplituden. Die Erweiterung der Loss-Funktion führte zu einer messbaren Verbesserung gegenüber der Standardvariante, ohne jedoch eine eindeutige Überlegenheit gegenüber Conformal Prediction zu etablieren.

\textbf{R2} Der entwickelte Fragenkatalog bietet eine strukturierte Übersicht relevanter Einflussgrößen auf das Erlernen von Unsicherheiten. Durch die systematische Anordnung lässt er sich sowohl als Referenz innerhalb des Projekts als auch für methodenübergreifende Vergleiche einsetzen.

\textbf{R3} Die gezielte Betrachtung ausgewählter Faktoren aus R2 verdeutlicht, dass Änderungen im Modellaufbau und bei den Trainingsparametern die Qualität der Unsicherheitsquantifizierung deutlich beeinflussen können. Die hinterlegten Quellenbezüge belegen diese Einschätzung und liefern Anhaltspunkte für eine Umsetzung.

\textbf{R4} Die Frage nach der Verlässlichkeit ML-basierter Unsicherheitsquantifizierung in realen Anwendungskontexten konnte in allgemeiner Form beantwortet werden. Für eine präzisere Einschätzung wären jedoch domänenspezifische Realdaten erforderlich, um methodische Grenzen und Stärken unter praxisnahen Bedingungen zu überprüfen.

\textbf{A1} Die Eingrenzung des Themenfeldes durch In- und Exklusionskriterien führte zu einer zielgerichteten Recherche. So wurde vermieden, dass die Recherche sich davon thematisch entfernt.

\textbf{A2} Die Darstellung grundlegender Unsicherheitsmetriken trug zu einem einheitlichen Verständnis für mich als Projektteilnehmer bei. Dieses Fundament erleichterte mir die methodische Gegenüberstellung und die Interpretation der Ergebnisse. Dies sollte dementsprechend auch für andere Projektteilnehmer gelten.

\textbf{A3} Die aufgeführten Anwendungsbeispiele aus dem Ingenieurwesen verdeutlichen den möglichen Transfer in der Bedeutung der vorgestellten Metriken. Die Kombination aus modell- und anwendungsspezifischen Aspekten schafft eine Grundlage für eine Auswahl von Ansätzen im Ingenieurwesen.

\textbf{A4} Der Vergleich zwischen Bayesischen und Evidentiellen Neuronalen Netzen sowie Conformal Prediction machte deutlich, dass die Methoden je nach Szenario unterschiedliche Vorteile bieten. Diese Erkenntnis kann als Entscheidungshilfe bei der Modellauswahl dienen.

\textbf{A5} Josangs \glqq{}Subjective Logic\grqq{} wird in meinen Augen dem in der Literatur angemerktem Robustheitsanspruch für sicherheitskritische Systeme gerecht, da eine eindeutige Logik-Hierarchie vorgegeben wird. Diese wurde an relevant empfundenen Stellen aufgezeigt. Sie würden in einer weiterführenden Arbeit den Transfer auf Evidentielle Neuronale Netzwerke im Hinblick auf eine Meta-Unsicherheit erlauben, sollte dieser weiter im Projekt verfolgt werden.

\textbf{A6} Die Experimente zeigten, dass sich mit zunehmender Stichprobengröße und höherer Epochenzahl deutliche Konvergenzeffekte zwischen Trainings- und Validierungsloss einstellen. Die aleatorische Varianz entsprach in den Untersuchungen linear dem Eingangsverhalten, sofern der ganze Raum untersucht wurde ohne eingeführten ZER-Operator. Die epistmische Varianz stellte sich in den Untersuchungen überwiegend als höher, als die aleatorische Varianz heraus. Zudem wurde deutlich, dass sich die Amplituden der Unsicherheiten gezielt an Bereiche anpassen lassen, was für weiterführende Studien eine relevante Stellgröße darstellen kann. Ein Über- und Unterfit wurden in $\gamma$ gegenübergestellt und verbundene aleatorische, epistemische Varianz aufgezeigt. Mit der erarbeiteten Trennung von Approximation und Fit wäre es angebracht, den Trainings- und Validierungsloss von VaMAIs Ziel niedriger Samplezahlen aufzulockern, um mit extremeren Loss-Konvergenzen über Trainings- und Validierungsdaten Ergebnisse zu erzielen, die einen Fit vom Ausgangssignal $y$ gegenüber $\gamma$ darstellen. Dieses Verständnis kann im Verbund mit weiteren Aspekten zur vollständigen Fit-Definition für evidentielle Netzwerke in VaMAI beitragen. Zur Ausarbeitung einer normativen Skala für epistemische und aleatorische Varianz, die über Linearitätsbeobachtungen hinausgeht, wäre es in meinen Augen sinnvoll, diese mit der Raumabdeckung aus in Beziehung zu setzen. Ebenso wären bei der Ausarbeitung der Plots für Vorhersagen zu Stichproben ein weiterer Schritt das Einführen von Quantilen, zum expliziteren erfassen der Abdeckung der Ground-Truth. Dies wäre in Relation zu setzen zu den Amplituden-Ausschlägen der aleatorischen und epistemischen Varianz. Beobachtet wurden sowohl Änderungen in Position als auch im Volumen für epistemische, als auch aleatorische Varianz. Hier wäre eine normierende Methode für eine weitere Ausarbeitung sinnvoll. Dies würde es erlauben die Gesamtunsicherheit über den Kritikalitätsraum gemäß der Eingangsformel der Gesamtunsicherheit additiv und normiert zusammenzusetzen und im Idealfall eindeutig an der Ground-Truth zu belegen.


% Bewertung der eingesetzten Methoden
\paragraph{Bewertung der Modellierungsansätze}

% Kommentiere hier z. B. die Eigenschaften deiner Loss-Funktion, inklusive des Clippings mit $U^*(x)$
Die Kombination aus evidenzbasierter Verlustfunktion und kritikalitätsabhängiger Begrenzung der Unsicherheit durch $U^*(x)$ bietet einen stabilen Ansatz zur Kontrolle von Überkonfidenz in sensitiven Regionen. Die eingeführte Lipschitz-Begrenzung durch die min-Funktion verhindert stark ansteigende Gradienten in extremen Fehlersituationen.  Allerdings kann dies auch dazu führen, dass in niedrig-kritischen Bereichen das Modell zu konservativ wird.

% Ggf. Unsicherheiten oder Regularisierung stärker hervorheben
Die Regularisierungsparameter $\nu$, $\alpha$ und $\beta$ sind dabei sensitiv in ihrer Wirkung. Insbesondere die Wahl von $\beta$ beeinflusst das Verhalten der Gesamtvarianz und sollte für reale Anwendungen datenspezifisch validiert werden.

% Umgang mit verschiedenen Unsicherheiten
\paragraph{Unsicherheitsquantifizierung im Kontext realer Systeme}

% Diskussion aleatorisch vs. epistemisch
Die Trennung von \gls{Aleatorische Unsicherheit} und \gls{Epistemische Unsicherheit} erfolgt hier auf Basis der evidenzbasierten Wahrscheinlichkeitsmodellierung. Dies ist theoretisch konsistent mit der Unsicherheitsdefinition der GUM, jedoch praktisch schwer zu validieren, solange keine realen Messdaten mit bekannter Varianzstruktur vorliegen.

% Synthetische Daten hinterfragen
Die genutzten Kritikalitätsfunktionen sind synthetisch definiert. Somit bleibt offen, ob das trainierte Netz diese Trennung bei realen, verrauschten Daten gleichermaßen leisten kann. Eine Erweiterung um reale Sensorikdaten wäre hier ein nächster Schritt.

\paragraph{Einfluss der Kritikalitätsfunktion und Regularisierung}

% Diskussion über die Konstruktion der Kritikalitätsräume
Die Kritikalitätsfunktion $C(x)$ wurde explizit modelliert, um Unsicherheiten in sensiblen Bereichen zu begrenzen. Diese Eingrenzung erweist sich als effektives Steuerungsinstrument im Trainingsprozess, stellt aber eine idealisierte Repräsentation realer Risiken dar. In praktischen Anwendungen müsste $C(x)$ durch Expertenwissen ersetzt werden.

% Hier kannst du auch die Rolle der Zero-Evidence-Regionen erwähnen
Ergänzend wurde eine ZER-Regelung über einen unteren Varianz-Schwellenwert eingeführt, um Regionen mit extrem geringer Evidenz explizit zu markieren. Dies erhöht die Erklärbarkeit des Modells, setzt aber voraus, dass die Parametrisierung der Evidenzverteilung sinnvoll gewählt wurde.

\paragraph{Limitationen}

% Offen und ehrlich: Was konnte nicht umgesetzt werden?
Ein wesentlicher Nachteil der durchgeführten Studie ist die fehlende Validierung mit realen Anwendungsdaten. Weder das Verhalten der Kritikalitätsfunktion noch die Effektivität der Verlustbegrenzung wurden im produktiven Kontext getestet. 
% Meta-Unsicherheit als Konzept zwar motiviert, jedoch nicht quantitativ experimentell untersucht.

% Weitere Punkte
Die gesamte Arbeit basiert auf synthetisch erzeugten Datensätzen. Dies ermöglicht zwar kontrollierte Experimente, schränkt aber die Generalisierbarkeit auf reale Szenarien ein.

\paragraph{Ausblick und Weiterentwicklungsmöglichkeiten}

% Wo könnte die Arbeit hinführen?
Ein möglicher nächster Schritt wäre die Übertragung der entwickelten Methoden auf reale Daten mit bekannter Unsicherheitsstruktur, zum Beispiel aus der Sensorik, Messtechnik oder dem Bereich autonomer Systeme. 
Auch eine Erweiterung des \gls{Evidenzbasierte neuronale Netze} um Rekurrenzmechanismen zum Beispiel LSTM zur Modellierung von Zeitdynamik wäre in begrenzter Abstraktion von Signalen denkbar.

% Meta-Unsicherheit als Forschungsrichtung
Ein weiteres Forschungspotenzial liegt in der quantitativen Ausführung des Konzepts der Meta-Unsicherheit – also der Unsicherheit über die Unsicherheitsmaße selbst. Hier könnten wiedersprüchliche Unsicherheitsquantifizierungen gegeneinander verglichen werden, um eine eindeutige Aussage zu treffen. 

% Letzter Satz: Stärke der Arbeit betonen
Die Arbeit zeigt, dass eine evidenzbasierte Unsicherheitsquantifizierung mit kritikalitätsabhängiger Regularisierung ein vielversprechender Ansatz zur Risikominimierung in sicherheitsrelevanten Systemen sein kann.



\end{otherlanguage}